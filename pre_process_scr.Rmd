---
title: "pre_process SCR shimmer data - social emophys"
author: "Helio"
date: "2024-03-07"
output: html_document
---

# Overview 
Starts with a clean environment
In this file I aim to process all the gsr data, such that 
1. we have a single full gsr recording per participant and trial
2. add psychopy event triggers
3. pass to ledalab to go gsr event related analysis
4. summarise and add meta-data

questions what is all calibrated folder - just one file
<!-- what5s the diferencee with the individual folders -->




note there were some issues with loading files documented in pre_process_scr_debug and
https://github.com/hcuve/social_emophys/issues/1

Set-up: where I load libraries and create objects for later use (such as color schemes etc).
```{r setup, include=FALSE}
# Load necessary libraries
require(tidyverse)
require(data.table)
library(janitor)
library(purrr)

# Define paths
data_path <- "./Data backup/DataChecks/R code and data/feb 6"
ctrl_data_path <- file.path(data_path, "ctrl")


```


load files 

```{r}
# Custom function to read a file and adjust its column names
fn_read_and_adjust <- function(file_path) {
  # Read the first two rows to adjust column names
  header_info <- fread(file_path, nrows = 1, skip = 1, header = FALSE)
  col_names <- colnames(fread(file_path, nrows = 1, skip = 2, header = FALSE))
  
  # Modify column names based on the header info
  new_col_names <- paste0(col_names, "_", unlist(header_info[1,]))
  
  # Now read the full data (skipping the first two rows) and assign new column names
  data <- fread(file_path, skip = 2, header = FALSE)
  setnames(data, old = colnames(data), new = new_col_names)
  
  # Return the modified data.table
  return(data)
}



```

recursive folder search
- check the list for all gsr files (calibrated) for sad and PC anmd count them


???? not sure we need this section

```{r eval=FALSE, include=FALSE}
# base_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio"

base_directory<- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys"

# Ensure this directory exists and R can access it. If there are access issues, consider absolute paths or check permissions.

# List immediate subdirectories first (non-recursive)
subdirs <- list.dirs(path = base_directory, recursive = FALSE, full.names = TRUE)

subdirs



# Initialize an empty vector to store the paths of all desired CSV files
csv_files_paths <- c()

# For each "Export" directory, look one level deeper
for (export_dir in export_dirs) {
    # List subdirectories within the current "Export" directory
    subdirs_in_export <- list.dirs(path = export_dir, recursive = FALSE, full.names = TRUE)
    
    # Filter out directories that contain "uncalibrated" in their name
    filtered_subdirs <- grep("uncalibrated", subdirs_in_export, value = TRUE, invert = TRUE, ignore.case = TRUE)
    
    # For each filtered subdirectory, list CSV files
    for (subdir in filtered_subdirs) {
        # List all CSV files in the subdirectory
        csv_files_in_subdir <- list.files(path = subdir, pattern = "\\.csv$", full.names = TRUE)
        
        # Add the found files to the collection
        csv_files_paths <- c(csv_files_paths, csv_files_in_subdir)
    }
}

# Proceed with applying the custom function to the list of CSV files

# Apply the custom function to each CSV file and combine the results

# why is 217 000



```


simpler approach 
- search on mac and copy easily to a folder called calibrated
- note check logbook for notes on data issues


GSR data should be sampled at 128 hz 

# 1000 ms = 128
# 6000 ms = 128*6 = 768
```{r}

setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR")


 # List of .csv files, fromm pc and sd

tmp_files_pc <- list.files(pattern = "GSR.*PC\\.csv$", 
                           recursive = FALSE, full.names = TRUE)
unique(tmp_files_pc)

tmp_files_sd <- list.files(pattern = "GSR.*SD\\.csv$", 
                           recursive = FALSE, 
                           full.names = TRUE)

unique(tmp_files_pc)
unique(tmp_files_sd)


```
tmp_files_pc
tmp_files_sd


# Extract identifiers and capture the session
```{r}
# Regular expression to capture the pattern before "_" and after "./"
 # "./101_Session1_GSRPulse_Calibrated_PC.csv"
pattern <- "\\./([0-9]+)_.*"



# Extract the pattern
# note sessions will likely differ between pc and SD (PC might more have more sessions fore the same data contained in one session of SD)
# SD might also contain data that doesn exist in PC (e.g. was deleted for not containing relevant exp info)
# focus identifiers on participants


identifiers_tmp_files_pc <- str_extract(tmp_files_pc, "\\d+")


length(unique(identifiers_tmp_files_pc)) 
# 208 dropped out
unique(identifiers_tmp_files_pc)
unique(identifiers_tmp_files_pc)
length(unique(identifiers_tmp_files_pc)) == 98 #TRUE

identifiers_tmp_files_sd <- str_extract(tmp_files_sd, "\\d+")
  # below stores sessions

length(unique(identifiers_tmp_files_sd)) == 95 #TRUE

# we should have 98-3 because the first 3 didn have sd data
identifiers_tmp_files_pc_sess <- sub("^\\./(.+)_GSR.*$", "\\1", tmp_files_pc)
identifiers_tmp_files_sd_sess <- sub("^\\./(.+)_GSR.*$", "\\1", tmp_files_sd)

# Compare and find common identifiers
# this telle us who ahs both pc and sd
common_identifiers <- intersect(identifiers_tmp_files_pc, identifiers_tmp_files_sd)
# should be 95

length(common_identifiers) == 95 #TRUE

#

```

# Identifiers unique to PC files
# rm(unique_identifiers_PC, unique_identifiers_SD)

```{r}
unique_identifiers_pc <- setdiff(identifiers_tmp_files_pc, identifiers_tmp_files_sd)
# "101" "102" "103" this is because we didn record SD
unique_identifiers_pc

unique_identifiers_sd <- setdiff(identifiers_tmp_files_sd, identifiers_tmp_files_pc)
unique_identifiers_sd


rm(tmp1, tmp2, tmp3)


```


check if we have SD for all 104 - 150
all 200s

if that's the case then just use PC for 101-103 and SDs for all the rest = done, go for this



###################################################
# Handling file imports from PC and SD sources:
# - This process primarily relies on SD files, but switches to PC files in the absence of a corresponding SD file.
# - A notable discrepancy has been observed in the export formats: PC exports contain "sync" within the timestamps, unlike SD exports. This difference may affect how filenames are generated and could imply variations in how data synchronization is handled or represented between the two sources.
# - It is essential to consider this variation during data processing to ensure consistent naming conventions and to investigate the synchronization status of SD data.
# - The custom function is applied to each file to adjust for these discrepancies, and the results are subsequently combined.

###################################################

tmp_files <- list.files(pattern = "GSR.*PC\\.csv$", recursive = FALSE, full.names = TRUE)
tmp_files

# do we actually want to store all of thes efiles here, or just load each adjut then save it back due to memory issues
# try again

# should have removed this before running the code below
# rm(dta_gsr)

# Custom function to read a file and adjust its column names
# Function to read a dataset and adjust column names based on the first two rows
# Overview:
# 1. Reads the first two rows of the file to use as basis for creating unique column names.
# 2. Combines these two rows to form a single set of column names for the dataset.
# 3. Reads the entire dataset, skipping the first two rows which were used for column names.
# 4. Assigns the combined column names to the dataset.
# 5. Adds a column indicating the filename from which the data was read.
# 6. This approach ensures proper data type handling by skipping header rows during the full data read.


fn_read_and_adjust <- function(file_path) {
  # Read the first two rows for column names adjustment
  header_info <- fread(file_path, nrows = 2, header = FALSE)
  
  # Combine the first two rows to form the column names, enhancing uniqueness
  col_names <- paste0(unlist(header_info[1,]), "_", unlist(header_info[2,]))
  
  # Read the full dataset, explicitly skipping the first two header rows
  data <- fread(file_path, skip = 3, header = FALSE) #using 3 just to be sure
  setnames(data, old = colnames(data), new = col_names) # Assign new column names
  
  # Add a column with the filename to keep track of the data source
  data[, filename := basename(file_path)]
  
  return(data)
}


note this below is not quite for ledalab  -see ahead
the sessio below depends on dta from psychopy?

```{r}


# Combine the lists of files


all_files <- c(tmp_files_pc, tmp_files_sd)

all_files


# Extract information: identifier and source (PC/SD)
# Create a data frame to hold file information
file_info <- data.frame(
  file_path = all_files,
  file_size = file.info(all_files)$size,
  identifier_ssid = str_extract(all_files, "\\d+"),
    # sub("^(.*)_GSR.*\\.csv$", "\\1", basename(all_files)),
  
  identifier_wsess = sub("^(.*)_GSR.*\\.csv$", "\\1", basename(all_files)),
  
  source = ifelse(grepl("PC\\.csv$", all_files), "PC", "SD")
)
file_info


file_info$keep_drop<- if_else(as.numeric(file_info$identifier_ssid) <104 & 
                              file_info$source == "PC", "keep",
                         if_else(file_info$source == "SD" & as.numeric(file_info$identifier_ssid) >103, "keep",
                                 "drop"))

table(file_info$keep_drop)

file_info_keep<- file_info%>%
  subset(keep_drop == "keep")

length(unique(file_info_keep$identifier_ssid)) # should be 98

length(unique(file_info_keep$identifier_wsess)) 

length(unique(file_info_keep$identifier_wsess)) 

unique(file_info_keep$identifier_wsess)

options(scipen = 999)
file_info_keep%>%
  group_by(identifier_ssid)%>%
  summarise_at(c("file_size"), sum)%>%
  arrange(file_size)
  ggplot(aes(file_size))+
  geom_histogram()
  
  file_info_keep
```


file_info_keep

# Predefined column names
final_files_to_load


colnames(dta_gsr[,2:12])
gsr_column_names <- colnames(dta_gsr[,2:12])
gsr_column_names
# 1] "gsr_pulse_timestamp_sync_unix_cal_ms"            
#  [2] "gsr_pulse_battery_cal_m_v"                       
#  [3] "gsr_pulse_gsr_range_cal_no_units"                
#  [4] "gsr_pulse_gsr_skin_conductance_cal_u_s"          
#  [5] "gsr_pulse_gsr_skin_resistance_cal_k_ohms"        
#  [6] "gsr_pulse_ppg_a13_cal_m_v"                       
#  [7] "gsr_pulse_ppg_ibi_cal_ms"                        
#  [8] "gsr_pulse_pp_gto_hr_cal_bpm"                     
#  [9] "gsr_pulse_pressure_bmp280_cal_k_pa"              
# [10] "gsr_pulse_temperature_bmp280_cal_degrees_celsius"
# [11] "gsr_pulse_event_marker_cal_no_units"

# Predefined column names
```{r}
gsr_column_names <- c(
  "gsr_pulse_timestamp_sync_unix_cal_ms",
  "gsr_pulse_battery_cal_m_v",
  "gsr_pulse_gsr_range_cal_no_units",
  "gsr_pulse_gsr_skin_conductance_cal_u_s",
  "gsr_pulse_gsr_skin_resistance_cal_k_ohms",
  "gsr_pulse_ppg_a13_cal_m_v",
  "gsr_pulse_ppg_ibi_cal_ms",
  "gsr_pulse_pp_gto_hr_cal_bpm",
  "gsr_pulse_pressure_bmp280_cal_k_pa",
  "gsr_pulse_temperature_bmp280_cal_degrees_celsius",
  "gsr_pulse_event_marker_cal_no_units"
)

# Print the vector
print(gsr_column_names)


```




# Helper function to extract participant ID and session from filename

```{r}
extract_info <- function(filename) {
  parts <- str_match(filename, ".*/(\\d+)_Session(\\d+)_GSRPulse_.*\\.csv$")
  return(list(participant_id = parts[, 2], session = parts[, 3]))
}

# Create a dataframe to map participant IDs and sessions to file paths
# flatten(final_files_to_load$file_path)
file_info_2 <- sapply(file_info_keep$file_path, 
                    function(x) extract_info(x), 
                    simplify = FALSE)
file_info_2


# creates a dataframe
file_df <- do.call(rbind, 
                   lapply(file_info_2, function(x) data.frame(participant_id = 
                                                              x$participant_id, 
                                                            session = x$session, stringsAsFactors = FALSE)))


file_df$filepath <- file_info_keep$file_path
file_df$file_size <- file_info_keep$file_size
# Process a list of file paths, adjust their column names, and combine into a single data.table
# Clean column names using `janitor::clean_names` for consistency
# Assuming tmp_files is a list of file paths
# file_df$participant_id = file_df$participant_id 
# participant_id = 136
rm(participant_id)

file_df

unique(file_df$participant_id)

file_df
```



```{r}
file_df$participant_id
file_df$session


dta_psypy1 <- readRDS("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/behavioural/preprocessed/dta_psypy1.rds")




colnames(dta_psypy1)
# if you don run this te subsetting breaks
dta_psypy1$orig_unixtime_start_psypy<- dta_psypy1$unixtime_start
setDT(dta_psypy1)


# reads from
dir_gsr<- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR"


f# exports to 
dir_gsr_w_triggers<- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR/gsr_w_triggers"



unique(file_df$participant_id)
# participant_id =140
# 140 is not exporting properly below because all their values aare consatnt in uni columns

for (participant_id in unique(file_df$participant_id)) {
  # Find all files for this participant
  participant_files <- file_df[file_df$participant_id == participant_id, ]
  
  # Initialize an empty list to hold session data
  all_sessions_data <- list()
  
  for (i in 1:nrow(participant_files)) {
    
    message("\nProcessing Participant ID: ", participant_id)
    # Load and process each file
    filename <- participant_files$filepath[i]
    session <- participant_files$session[i]
    
     # Construct the full file path
    filepath <- file.path(dir_gsr, filename)

    # Read the CSV file, skip the first 3 rows
    tmp_gsr <- fread(filepath, skip = 3, header = FALSE)
    
    # Check if the dataframe has 10 columns, and if so, add an empty 11th column
if (ncol(tmp_gsr) == 10) {
  # Add an 11th column filled with NA
  tmp_gsr[, V11 := NA]
}

# Proceed with removing extra columns if necessary
extra_cols <- ncol(tmp_gsr) - length(gsr_column_names)
if (extra_cols > 0) {
  tmp_gsr <- tmp_gsr[, -((ncol(tmp_gsr)-extra_cols+1):ncol(tmp_gsr)), with = FALSE]
}

# Now assign predefined column names, assuming 'gsr_column_names' is of length 11
setnames(tmp_gsr, old = names(tmp_gsr), new = gsr_column_names)


    # Assign predefined column names
    setnames(tmp_gsr, old = names(tmp_gsr), new = gsr_column_names)
    
    # Add a new column with the filename
    filename <- basename(filepath)
    tmp_gsr[, filename := filename]
    
    tmp_gsr[, session := session]
    
    # Correct original timestamp column name to match GSRPulse data
    tmp_gsr[, orig_timestamp_gsr_pulse := gsr_pulse_timestamp_sync_unix_cal_ms]
    
    # Merge timestamps and other operations from dta_psypy1

    tmp_psypy <- dta_psypy1[participant == participant_id, 
                            .(participant, social_nonsocial, stim_iaps, trial_no_all, unixtime_start, orig_unixtime_start_psypy)]
    
    # Error: object 'participant' not found - this means you forgot to run  dta_psypy1, also checl for missing columns
    
    tmp_psypy[, next_unixtime_start := shift(unixtime_start, type = "lead", fill = Inf)]
    tmp_psypy[, unixtime_end := next_unixtime_start - 1]
    
    # Initialize missing columns in tmp_gsr
    tmp_gsr[, `:=` (social_nonsocial = as.character(NA), 
                         stim_iaps = as.character(NA), 
                         trial_no_all = as.integer(NA), 
                         psypy_unixtime_start = as.numeric(NA),
                         orig_unixtime_start_psypy = as.numeric(NA))]
    
    # Perform the non-equi join and update
    tmp_gsr[tmp_psypy, on = .(gsr_pulse_timestamp_sync_unix_cal_ms >= unixtime_start, 
                                   gsr_pulse_timestamp_sync_unix_cal_ms <= unixtime_end), 
                 `:=` (social_nonsocial = i.social_nonsocial, 
                       stim_iaps = i.stim_iaps, 
                       trial_no_all = i.trial_no_all, 
                       psypy_unixtime_start = i.unixtime_start,
                       orig_unixtime_start_psypy = i.orig_unixtime_start_psypy), 
                 by = .EACHI]
    
    # Append the processed session data to the list
    all_sessions_data[[length(all_sessions_data) + 1]] <- tmp_gsr
  }
  
  # Combine data from all sessions
  combined_data <- do.call(rbind, all_sessions_data)
  
  # Convert combined data to tibble for dplyr operations
  combined_data <- as_tibble(combined_data)
  
  # Apply dplyr operations
  combined_data <- combined_data %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%
    mutate(start_true = !duplicated(trial_no_all) & !is.na(trial_no_all),
           trigger = case_when(
             trial_no_all == 1 & start_true == TRUE ~ 1,
             start_true == FALSE ~ 0,
             TRUE ~ as.numeric(trial_no_all)),
           gsr_pulse_timerezero_sync_unix_cal_ms = gsr_pulse_timestamp_sync_unix_cal_ms - first(gsr_pulse_timestamp_sync_unix_cal_ms))%>%
    mutate(participant = participant_id)
  
# Just before writing out the combined_data
unique_triggers <- length(unique(combined_data$trigger))
unique_trigger_gt_zero <- combined_data %>% 
  filter(trigger > 0) %>%
  count(trigger) %>%
  filter(n > 1)

# Check for conditions
if (unique_triggers != 66 || nrow(unique_trigger_gt_zero) > 0) {
  warning_message <- paste("Warning: Data integrity checks failed for Participant ID:", participant_id)
  
  if (unique_triggers != 66) {
    warning_message <- paste(warning_message, "\nExpected 66 unique trigger values, but found", unique_triggers)
  }
  
  if (nrow(unique_trigger_gt_zero) > 0) {
    warning_message <- paste(warning_message, "\nFound trigger values greater than 0 that are present more than once:")
    for (i in 1:nrow(unique_trigger_gt_zero)) {
      warning_message <- paste(warning_message, unique_trigger_gt_zero$trigger[i], "(", unique_trigger_gt_zero$n[i], "times);")
    }
  }
  
  # Print the warning message
  message(warning_message)
} else {
  # If all checks pass, proceed with writing the file
  
  # dir_gsr_w_triggers
  # fwrite(as.data.table(combined_data), paste0(participant_id, "_allsessions", "_dta_gsr_pulse_trig.csv"), row.names = FALSE)
  
  output_file <- file.path(dir_gsr_w_triggers, paste0(participant_id, "_allsessions", "_dta_gsr_pulse_trig.csv"))
    fwrite(as.data.table(combined_data), output_file, row.names = FALSE)
    
  
  # Print completion message for current participant
  message("Completed processing for Participant ID: ", participant_id)
}
}
# this results in 97 files rather than 98


```
Participant ID: 208 - didnt complete the task - remebver to exclude them
Warning: Data integrity checks failed for Participant ID: 208 
Expected 66 unique trigger values, but found 32



Just some checks
```{r}

unique(tmp_gsr$trial_no_all)

# tmp_gsr<-  read_csv("136_allsessions_dta_gsr_pulse_trig.csv")

colnames(combined_data)
combined_data%>%
  # subset(participant == 121)%>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%

    # subset(trial_no_all>10 & trial_no_all<18)%>%
  mutate(time_diff_psypy_gsr = orig_unixtime_start_psypy-orig_timestamp_gsr_pulse)%>%
  ggplot( aes(x = gsr_pulse_timestamp_sync_unix_cal_ms, 
                         y = gsr_pulse_gsr_skin_conductance_cal_u_s)) +
  geom_line() 
 

```




write to ledalab

# Data Preparation for Ledalab Analysis
# Note: 
# - Ledalab has very specific format requirements that cna create issues= needs .txt without column names.
# - The presence of 'NA' values, weird characters, or numbers as characters (e.g "1") even empty spaces can cause loading, opening, or importing issues within Ledalab. It may lead to repeated prompts for specifying variables, which indicates these common problems.
# - It is crucial to meticulously prepare and clean the data to meet these specific requirements to avoid any operational disruptions in Ledalab.

# Comparative Validation Note:
# - A manual examination was conducted between a dataset processed manually (participant 110) and another processed through this automated script to gtenearte trigger. This was done to ensure that the start times, particularly for GSR (Galvanic Skin Response) data, accurately match when analyzed in Ledalab.
# - This comparison confirmed that both the timing and the pattern of analysis remain consistent between manually processed and automated datasets. 


for ledalab, we'll just write time stamp (original and re zeroed, skin conductance data) and triggers, anyt medatdasta wil be matched later - based on trigger (consider writing an aggregate matching file from trigger to psypy?)

this one below export the fulls dataset, but its best to export rest and trials separatelly

140 has an issue
# List all CSV files in the directory
gsr_wtrigg_files <- list.files(dir_gsr_w_triggers, 
                                pattern = "\\.csv$", full.names = TRUE)

gsr_wtrigg_files


test_a<- unique(substr(list.files(dir_gsr_w_triggers, 
                                pattern = "\\.csv$", full.names = F),1,3))

test_b<- unique(file_df$participant_id)


intersect(test_a, test_b)
setdiff(test_a, test_b)

setdiff(test_b,test_a)
```{r eval=FALSE, include=FALSE}

library(dplyr)
library(tools)

# Define the directory where participant CSV files are stored
setwd(dir_gsr_w_triggers)


# Define the output directory for the formatted files
dir_4leda <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR/gsr_w_triggers/forledalab/"




# Set the input and output directories
# input dire dir_gsr_w_triggers
# output dir dir_4leda

# Function to process each file

fn_process_gsr_wtrigg_files <- function(file_path) {
  # Read the CSV file
  data <- read.csv(file_path) %>%
    select(gsr_pulse_timerezero_sync_unix_cal_ms,
           gsr_pulse_timestamp_sync_unix_cal_ms,
           gsr_pulse_gsr_skin_conductance_cal_u_s,
           trigger)
  
  # Arrange by gsr_pulse_timestamp_sync_unix_cal_ms
  selected_data <- data %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms)
  
  # Construct output file name based on input file name and output directory
  file_name_no_ext <- tools::file_path_sans_ext(basename(file_path))
  output_file_name <- file.path(dir_4leda, paste0(file_name_no_ext, "_ledaform.txt"))
  
  # Write the selected and arranged data to a new file, formatted for Ledalab
  write.table(selected_data, 
              output_file_name, sep = "\t",
              row.names = FALSE, 
              col.names = FALSE)
  
  # Print a message indicating progress
  cat("Processed and written:", output_file_name, "\n")
}


# Apply the function to each file
lapply(gsr_wtrigg_files, fn_process_gsr_wtrigg_files)
```


export trials and rest period separatelly
```{r}
dir_4leda <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR/gsr_w_triggers/forledalab/"



setwd(dir_gsr_w_triggers)

fn_process_rest_vs_pratc_trials <- function(file_path) {
  # Read the CSV file and select relevant columns
  data <- read.csv(file_path) %>%
    select(gsr_pulse_timerezero_sync_unix_cal_ms,
           gsr_pulse_timestamp_sync_unix_cal_ms,
           gsr_pulse_gsr_skin_conductance_cal_u_s,
           trigger) %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms)

  # Find indices for trigger conditions
  first_trigger_2_index <- which(data$trigger == 2)[1]
  first_trigger_1_index <- which(data$trigger == 1)[1]

  # Function to rezero time
  rezero_time <- function(df) {
    df %>%
      mutate(gsr_pulse_timerezero_sync_unix_cal_ms = gsr_pulse_timerezero_sync_unix_cal_ms -
               gsr_pulse_timerezero_sync_unix_cal_ms[1])
  }

  # Data before first occurrence of trigger == 2
  if (!is.na(first_trigger_2_index) && first_trigger_2_index > 0) {
    data_before_trigger_2 <- data %>%
      slice(1:(first_trigger_2_index - 1)) %>%
      rezero_time()

    # Construct output file name for data before trigger == 2
    file_name_no_ext <- tools::file_path_sans_ext(basename(file_path))
    output_file_name_before_trigger_2 <- paste0(dir_4leda, "/", file_name_no_ext, "_rest_ledaform.txt")

    # Write to file
    write.table(data_before_trigger_2, output_file_name_before_trigger_2, sep = "\t", row.names = FALSE, col.names = FALSE)
    cat("Processed and written (before trigger 2):", output_file_name_before_trigger_2, "\n")
  }

  # Data after first occurrence of trigger == 1, excluding the trigger row itself
  if (!is.na(first_trigger_1_index) && first_trigger_1_index > 0) {
    data_after_trigger_1 <- data %>%
      slice((first_trigger_1_index + 1):n()) %>%
      rezero_time()

    # Construct output file name for data after trigger == 1
    output_file_name_after_trigger_1 <- paste0(dir_4leda, "/", file_name_no_ext, "_pract_nd_trials_ledaform.txt")

    # Write to file
    write.table(data_after_trigger_1, output_file_name_after_trigger_1, sep = "\t", row.names = FALSE, col.names = FALSE)
    cat("Processed and written (after trigger 1):", output_file_name_after_trigger_1, "\n")
  }
}


# Process each file
lapply(gsr_wtrigg_files, fn_process_rest_vs_pratc_trials)

```

LEDALAB
# ==============================================================================
# README: Preparation and Analysis of Ledalab Formatted Files
# ==============================================================================
ledaform files
GSR files were processed to select relevant columns and arrange data by timestamp, ensuring compatibility with Ledalab software. Output files are saved without column names, using tab-separated values (TSV), in line with Ledalab's input requirements. note ledalab has an issue wityh NAs, strringds, empty columsn or even numnumebrsebrs wrriten like "11".
any issues with loading likely has to do with that

### Output File Format:
Each file is named after the original participant data file, appended with "_ledaform.txt" to indicate formatting for Ledalab. The files include the following columns:
  1. **gsr_pulse_timerezero_sync_unix_cal_ms**: Timestamp (Unix time) after rezeroing, indicating synchronized GSR pulse time.
  2. **gsr_pulse_timestamp_sync_unix_cal_ms**: Original Unix timestamp of the GSR pulse.
  3. **gsr_pulse_gsr_skin_conductance_cal_u_s**: Calibrated skin conductance level in microsiemens (Î¼S).
  4. **trigger**: Event marker or trigger signal associated with the measurement.

### Sampling Rate and Downsampling:
- **Original Sampling Rate**: 128Hz.
- **Downsampling Target**: 16Hz (or the lowest rate that maintains a frequency above 10Hz), corresponding to a downsampling factor of 8 (128Hz / 8 = 16Hz).
- **Method**: Use "factor mean" in Ledalab for downsampling, averaging data points within each factor to preserve signal characteristics.

## Importing Data into Ledalab:
Select 'Data Type 3' (manual definition) upon import to manually specify data structure for accurate interpretation and analysis within Ledalab.

# Analysis Guidelines for Ledalab:

## Preliminary Manual Analysis (GUI):
Before batch processing, it's essential to manually analyze a one or two files to establish a benchmark for comparison and ensure the analysis settings are correctly configured. This will also serve as a referecne for automated analysis

### Steps for Manual Analysis:
1. **Open Ledalab's GUI**: Load a files from 'rest' and 'pract_nd_trial' categories to familiarize with the data and settings.
2. **Configure Analysis Settings**:
   - Enable **Adaptive Smoothing**: Adjusts the smoothing parameter dynamically for each data segment to improve the signal-to-noise ratio.

3. **Run CDA Analysis**: Perform Continuous Decomposition Analysis to decompose the GSR signal into its tonic and phasic components.
   - **Optimize Twice**: Enhances the accuracy of the Continuous Decomposition Analysis (CDA) by refining the decomposition parameters iteratively.
4. **Export event related activation**:
   - For trials: Analyze event-related data within a window of **1 to 7 seconds** (or **0.5 to 7 seconds** if permissible).
   - For rest: Use a window of **1 to 120 seconds** to capture the full range of resting data.
5. **Export and Save Results**: Ensure to save `.mat` files for the analysis results and key plots for later comparison
also save a mat file of the whole analysis

## Automated Batch Processing:
After manual analysis, automate the processing for all files using Ledalab's batch mode. This ensures consistent application of the predetermined settings across all files.

### Batch Processing Steps:
- Visit [Ledalab Batch Mode Documentation](http://www.ledalab.de/documentation.htm#batchmode) for detailed instructions on setting up batch processing.

#### Sample Batch Processing Commands: check the slashes as t5hey differ on windows and mac

- **For Practice and Trials**:
Ledalab('D:\OneDrive - Nexus365\InteroStudy2020\analysis\DataAnalysisJanuary2020\DataAnalysisJan2020\GP_gsr_forLeda', 'open','text3','downsample', 15, 'smooth',{'adapt'}, 'analyze','CDA', 'optimize',3, 'export_era', [1 7 .01 2],  'overview',1)

Code for automatic rest analysis
Ledalab('D:\OneDrive - Nexus365\InteroStudy2020\analysis\DataAnalysisJanuary2020\DataAnalysisJan2020\GP_gsr_forLeda', 'open','text3','downsample', 15, 'smooth',{'adapt'}, 'analyze','CDA', 'optimize',3, 'export_era', [1 120 .01 2],  'overview',1)


inspect some fo the saved outputs and plots and list of problem files.
for any files you can't run in batch mode run manualy


combine the ledalab output with behavioural data and check if we have what we would expect

3. Plot Physio data in R and Ledalab

now load ledalab data


```{r}
# Set the working directory to where your files are located
setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/RData/leda")

# List all files with a .txt extension
file_list <- list.files(pattern = "\\.txt$")

# Use lapply to read each file and then combine them with do.call and rbind
# dta_leda_era <-  Read files, adding a column with the filename
dta_leda_era <- do.call(rbind, lapply(file_list, function(file) {
  data <- read.table(file, header = TRUE, sep = "\t")%>%
       janitor::clean_names()
  # Add a new column to store the filename
  data$filename <- file
  
  return(data)
}))
  


dta_leda_era
  


# all_data now contains the combined data from all files
dta_psypy1$trial_no_all
unique( dta_leda_era$event_name)
dta_leda_era$trial_no_all<- dta_leda_era$event_name

dta_leda_era$filename
dta_leda_era$participant<- substr(dta_leda_era$filename, 1,3)
unique(dta_leda_era$participant)
colnames(dta_leda_era)
dta_psypy1$trial_no_all

colnames(dta_leda_era)
dta_leda_era1<- dta_leda_era%>%
  group_by(participant)%>%
  arrange(participant,trial_no_all)%>%
  mutate(prop_gsr_resp_ppt = mean(sum(cda_n_scr>0)/65))%>%
  select(participant, trial_no_all,cda_n_scr,cda_amp_sum,cda_phasic_max,cda_scr,
         cda_iscr,prop_gsr_resp_ppt)
  

colnames(dta_leda_era1)

range(dta_leda_era1$prop_gsr_resp_ppt)

dta_leda_era1%>%
  group_by(participant)%>%
  summarise_if(is.numeric, mean, na.rm = T)%>%
  ggplot(aes(prop_gsr_resp_ppt))+
  geom_histogram()



dta_leda_era1
```


save outputs

```{r}
setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR")

saveRDS(dta_leda_era, "dta_gsr_leda_era.rds")
save.image("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR/dta_gsr_env.RData")

```



Quick checks that the gsr data sent for students matches the gsr data that we re-ran

```{r}
# just one pp

setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR/gsr_w_triggers/forledalab/Ledalab output")


dta_gsr_leda_era_1 <- readRDS("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR/gsr_w_triggers/forledalab/Ledalab output/dta_gsr_leda_era_1.rds")


dta_gsr_leda_era_1
colnames(dta_gsr_leda_era)

unique(dta_gsr_leda_era$filename)

unique(dta_gsr_leda_era$trial_no_all)


rm(dta_gsr)

setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR/gsr_w_triggers/forledalab/Ledalab output/compare/New")

file_list_new <- list.files(pattern = "\\.txt$")


tmp_gsr_new<- do.call(rbind, lapply(file_list_new, function(file) {
  data <- read.table(file, header = TRUE, sep = "\t")%>%
       janitor::clean_names()
  # Add a new column to store the filename
  data$filename <- file
  
  return(data)
}))



setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Re-exported Consensys/organised for R/Calibrated/GSR/gsr_w_triggers/forledalab/Ledalab output/compare/old")

file_list_old <- list.files(pattern = "\\.txt$")

tmp_gsr_old<- do.call(rbind, lapply(file_list_old, function(file) {
  data <- read.table(file, header = TRUE, sep = "\t")%>%
       janitor::clean_names()
  # Add a new column to store the filename
  data$filename <- file
  
  return(data)
}))

unique(tmp_gsr_new$filename)

unique(tmp_gsr_new$event_name)

unique(tmp_gsr_old$event_name)

colnames(tmp_gsr_new)
left_join(tmp_gsr_new,
          tmp_gsr_old, by = c("filename", "event_name"))%>%
  ggplot(aes(cda_n_scr.x, cda_n_scr.y))+
  geom_point()+
  geom_smooth(aes(group = filename), method = "lm", se = F)+
  ggpubr::stat_cor()+
  facet_grid(~filename)



left_join(tmp_gsr_new,
          tmp_gsr_old, by = c("filename", "event_name"))%>%
  ggplot(aes(global_mean.x, global_mean.y))+
  geom_point()+
  geom_smooth(aes(group = filename), method = "lm", se = F)+
  ggpubr::stat_cor()+
  facet_grid(~filename)


colnames(tmp_gsr_new)
left_join(tmp_gsr_new,
          tmp_gsr_old, by = c("filename", "event_name"))%>%
  ggplot(aes(ttp_amp_sum.x, ttp_amp_sum.y))+
  geom_point()+
  geom_smooth(aes(group = filename), method = "lm", se = F)+
  ggpubr::stat_cor()+
  facet_grid(~filename)

# check confirms that the new processing didn't change the results
```
