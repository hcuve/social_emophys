---
title: "pre_process SCR shimmer data - social emophys"
author: "Helio"
date: "2024-03-07"
output: html_document
---



Understanding Data Inconsistencies and Solutions
1. Inconsistent Session Data Between SD and PC:

Issue: We've observed instances where data from the same time period are categorized under different sessions on SD and PC. 
Implication: It complicates data analysis, as the same physical events may be represented in separate session files, causing confusion and inaccuracies in interpreting participant behavior and responses.

2. Duplicate Participant IDs Across Different Individuals:

Issue: There are cases where files from different individuals have been assigned the same participant ID, 

3. Varied File Sizes and Incomplete Sessions:

Issue: Some files are significantly smaller than others, raising questions about their content and whether all sessions, including those where no actual data was recorded, have been retained.
Implication: Retaining sessions with no valuable data can clutter the dataset, making analysis more challenging and potentially skewing results.
Solution: Review the files to identify and exclude sessions with no meaningful data. This step will streamline the dataset, focusing analysis on relevant and substantive data points.
4. Data Processing Strategy for Tomorrow:

Immediate Task: Focus solely on Heart Rate (HR) data to compute mean values. This simplifies the immediate analysis scope, allowing for a focused approach to understanding the dataset's fundamental aspects.
Next Steps: Once HR data analysis is complete, the next phase will involve processing facial data, expanding the analysis scope to incorporate additional data dimensions.

5. Proposed Logic for Data Selection:
its imporatnt to addres the previous points and additionally below

- If both PC and SD data are available: We will choose the file with the larger size, as it is likely to contain the most complete dataset. if that indeed the valid data you stored
- If only PC data is available: We will use the PC file.
- If only SD data is available: We will use the SD file.


Ultimately, after5 each session we need to organize our data into folders containing usable, final files for each person for each signal type (e.g., all GSR data in a GSR folder). This organization will facilitate our analysis, ensuring we work with the most complete and accurate data possible.

when I am analyzing, I need to just have the files i need to load, and not having to sort out the logic of which file to use




Botwe to self
to deetct these isues use
- Data Summarization: We will calculate the total size of the data for each participant to help identify any substantial discrepancies that may indicate missing or incomplete data.
check nujmber of events in each file (should be 65 or 66)
- focus on j=redoing the gsr and use hrt fro this data fro the students

2. Export/create triggers from behavioral data (merge physio with psychopy)

COMBINE TRIGGERS WITH PHYSIO

we need top rerun 118 and 110 due to overlapping names
222 and 223

Set-up: where I load libraries and create objects for later use (such as color schemes etc).
```{r setup, include=FALSE}
# Load necessary libraries
require(tidyverse)
require(data.table)
library(janitor)
library(purrr)

# Define paths
data_path <- "./Data backup/DataChecks/R code and data/feb 6"
ctrl_data_path <- file.path(data_path, "ctrl")


```


load files 
```{r}



# Custom function to read a file and adjust its column names
fn_read_and_adjust <- function(file_path) {
  # Read the first two rows to adjust column names
  header_info <- fread(file_path, nrows = 1, skip = 1, header = FALSE)
  col_names <- colnames(fread(file_path, nrows = 1, skip = 2, header = FALSE))
  
  # Modify column names based on the header info
  new_col_names <- paste0(col_names, "_", unlist(header_info[1,]))
  
  # Now read the full data (skipping the first two rows) and assign new column names
  data <- fread(file_path, skip = 2, header = FALSE)
  setnames(data, old = colnames(data), new = new_col_names)
  
  # Return the modified data.table
  return(data)
}



```

recursive folder search
```{r eval=FALSE, include=FALSE}
base_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio"

# Ensure this directory exists and R can access it. If there are access issues, consider absolute paths or check permissions.
# List immediate subdirectories first (non-recursive)
subdirs <- list.dirs(path = base_directory, recursive = FALSE, full.names = TRUE)


# Initialize an empty vector to store the paths of all desired CSV files
csv_files_paths <- c()

# For each "Export" directory, look one level deeper
for (export_dir in export_dirs) {
    # List subdirectories within the current "Export" directory
    subdirs_in_export <- list.dirs(path = export_dir, recursive = FALSE, full.names = TRUE)
    
    # Filter out directories that contain "uncalibrated" in their name
    filtered_subdirs <- grep("uncalibrated", subdirs_in_export, value = TRUE, invert = TRUE, ignore.case = TRUE)
    
    # For each filtered subdirectory, list CSV files
    for (subdir in filtered_subdirs) {
        # List all CSV files in the subdirectory
        csv_files_in_subdir <- list.files(path = subdir, pattern = "\\.csv$", full.names = TRUE)
        
        # Add the found files to the collection
        csv_files_paths <- c(csv_files_paths, csv_files_in_subdir)
    }
}

# Proceed with applying the custom function to the list of CSV files

# Apply the custom function to each CSV file and combine the results

# grep("_PC",csv_files_paths)
# 
# tmp_test <-csv_files_paths[grep("_PC",csv_files_paths)]
# 
# tmp_test[grep("GSR",tmp_test)]

# combined_data <- rbindlist(lapply(csv_files_paths, fn_read_and_adjust), use.names = TRUE, fill = TRUE, idcol = "filename") %>%
#   janitor::clean_names()

# 'combined_data' now contains your adjusted and combined dataset



# why is 217 000



```


simpler approach 
- search on mac and copy easily to a folder called calibrated
- note check logbook for notes on data issues


GSR data should be sampled at 128 hz 

# 1000 ms = 128
# 6000 ms = 128*6 = 768
```{r}

setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/GSR")
# List of .csv files, fromm pc and sd

tmp_files_pc <- list.files(pattern = "GSR.*PC\\.csv$", recursive = FALSE, full.names = TRUE)
unique(tmp_files_pc)
tmp_files_sd <- list.files(pattern = "GSR.*SD\\.csv$", recursive = FALSE, full.names = TRUE)

tmp_files_pc
tmp_files_sd


# Extract identifiers and capture the session

# Regular expression to capture the pattern before "_" and after "./"
pattern <- "\\./([0-9]+)_.*"

# Extract the pattern
identifiers_tmp_files_pc <- sub("^\\./(.+)_GSR.*$", "\\1", tmp_files_pc)
identifiers_tmp_files_sd <- sub("^\\./(.+)_GSR.*$", "\\1", tmp_files_sd)

# Compare and find common identifiers
common_identifiers <- intersect(identifiers_tmp_files_pc, identifiers_tmp_files_sd)

# Display the common identifiers
common_identifiers

# Identifiers unique to PC files
# rm(unique_identifiers_PC, unique_identifiers_SD)
unique_identifiers_pc <- setdiff(identifiers_tmp_files_pc, identifiers_tmp_files_sd)
unique_identifiers_pc
#  [1] "101_Session1" "102_Session1" "103_Session3" "104_Session2" "106_Session2" "106_Session3" "107_Session2" "112_Session2" "118_Session2"
# [10] "119_Session3" "120_Session2" "120_Session3" "124_Session2" "126_Session2" "127_Session2" "127_Session3" "128_Session2" "131_Session2"
# [19] "135_Session2" "135_Session3" "201_Session3" "202_Session2" "210_Session2" "214_Session3" "214_Session4" "215_Session2" "217_Session2"
# [28] "220_Session2" "221_Session2" "223_Session4" "224_Session2" "227_Session2" "228_Session2" "228_Session4" "230_Session3" "232_Session2"
# [37] "233_Session5"
# Identifiers unique to SD files
unique_identifiers_sd <- setdiff(identifiers_tmp_files_sd, identifiers_tmp_files_pc)
unique_identifiers_sd


# now if just take the sid, how many people dow e have

identifiers_tmp_files_pc

# length(unique(gsub("_.*", "", identifiers_tmp_files_pc))) #69
# length(unique(gsub("_.*", "", identifiers_tmp_files_sd))) #66

tmp1<- unique(gsub("_.*", "", unique_identifiers_pc))
tmp2<-unique(gsub("_.*", "", unique_identifiers_sd))
tmp3<-unique(gsub("_.*", "", common_identifiers))

combined_vector <- c(tmp1, tmp2,tmp3) #this shoiuld be 70 but its  45, why
length(unique(combined_vector)) #70
# 36+34
# expecting 70 files
# Verify against your expectation

# now we want to create a list that contains tmp_files_pc plus the tmp_unique_identifiers_sd

# Assuming tmp_unique_identifiers_sd contains unique identifiers like c("101", "102", "106")
# Create a regular expression pattern that matches any of these identifiers
pattern_unique_sd <- paste(unique_identifiers_SD, collapse = "|")

# Use grep to select files from tmp_files_sd that match the unique identifiers
tmp_selected_files_sd <- tmp_files_sd[grep(pattern_unique_sd, tmp_files_sd)]

# Display the selected files
tmp_selected_files_sd

# Combine selected_files_sd with tmp_files_pc
tmp_combined_list <- c(tmp_files,tmp_selected_files_sd)

# Display the combined vector

tmp_combined_list
# remember to update 000_Session1_ECG_Calibrated_PC to 2176?

# so we are picking up all files
 grep("223", tmp_combined_list, value=TRUE)
 grep("223", tmp_files_sd, value=TRUE)

```

1. Prioritize Based on Total Data Volume
If both sources are present for a participant: Compare the total sizes. The source with a larger total size could be considered as having more complete data across sessions, thus, might be preferable.
If only one source is present for a participant: Use the available source, as it's the only option for that participant.
2. Implement Decision Logic
Based on the above interpretation, you can implement logic to automatically decide which source to prioritize for further analysis or processing. Here’s a sample approach:

a. Prefer the Source with More Complete Data Across Sessions
For participants with both PC and SD files, if the total size of SD files significantly exceeds that of PC files, this suggests that SD recordings might be more complete on average. This could be a basis to prefer SD over PC when both are available, especially if your analysis benefits from more comprehensive datasets.

b. Fallback to Available Data When Only One Source Exists
For participants with data only from one source, the decision is straightforward as you'll use what's available.

3. Code Example

the code below does not combine sessions PC or SD within a particiopant

```{r}
# Assuming tmp_files_pc and tmp_files_sd have already been defined and include full paths

# Combine the lists of files
length(tmp_files_pc)+length(tmp_files_sd) #161
all_files <- c(tmp_files_pc, tmp_files_sd)

# Extract information: identifier and source (PC/SD)
# Create a data frame to hold file information
file_info <- data.frame(
  file_path = all_files,
  file_size = file.info(all_files)$size,
  identifier = sub("^(.*)_GSR.*\\.csv$", "\\1", basename(all_files)),
  source = ifelse(grepl("PC\\.csv$", all_files), "PC", "SD")
)
# For each identifier, determine which file to keep
chosen_files <- lapply(unique(file_info$identifier), function(id) {
  subset_files <- subset(file_info, identifier == id)
  
  if (nrow(subset_files) == 1) {
    return(subset_files$file_path)
  } else {
    # Choose the file with the larger size
    return(subset_files$file_path[which.max(subset_files$file_size)])
  }
})

file_info$file_path

file_info$session =  sub(".*_(Session\\d+).*", "\\1", file_info$file_path)
file_info$participant = sub("./(.+?)_.*", "\\1", file_info$file_path)


total_size_per_participant_source<- file_info%>%
  group_by(participant, source)%>%
  summarise_at(c("file_size"), sum, na.rm = T)


# Pivot the data to have PC and SD sizes side by side for each participant
library(reshape2)

pivot_total_size <- dcast(total_size_per_participant_source, participant ~ source, value.var="file_size", fill=0)

# Determine the preferred source for each participant
pivot_total_size$preferred_source <- ifelse(pivot_total_size$SD > pivot_total_size$PC, "SD", "PC")

# Adjust preferred_source for participants with data only from one source
pivot_total_size$preferred_source <- ifelse(pivot_total_size$PC == 0 & pivot_total_size$SD > 0, "SD", pivot_total_size$preferred_source)
pivot_total_size$preferred_source <- ifelse(pivot_total_size$SD == 0 & pivot_total_size$PC > 0, "PC", pivot_total_size$preferred_source)

# Optional: For a strict comparison, in case both sizes are equal (unlikely but possible), decide based on a predefined preference
# This line is optional and can be adjusted based on your specific needs or omitted if not needed

pivot_total_size$preferred_source <- ifelse(pivot_total_size$PC == pivot_total_size$SD, "SD", pivot_total_size$preferred_source)

table(pivot_total_size$preferred_source)
# now load the files based on this preference


# Assuming pivot_total_size and file_info are already defined as shown in your code

# Merge the preference back to the original file_info dataframe to know which files to keep
file_info <- merge(file_info, pivot_total_size[, c("participant", "preferred_source")], by="participant")

# Filter out files based on the preferred source
final_files_to_load <- file_info[file_info$source == file_info$preferred_source,]



```

are there cases where we need to combine SD and PC?
load gsr data


setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated")
###################################################
# Handling file imports from PC and SD sources:
# - This process primarily relies on PC files, but switches to SD files in the absence of a corresponding PC file.
# - A notable discrepancy has been observed in the export formats: PC exports contain "sync" within the timestamps, unlike SD exports. This difference may affect how filenames are generated and could imply variations in how data synchronization is handled or represented between the two sources.
# - It is essential to consider this variation during data processing to ensure consistent naming conventions and to investigate the synchronization status of SD data.
# - The custom function is applied to each file to adjust for these discrepancies, and the results are subsequently combined.

###################################################

tmp_files <- list.files(pattern = "GSR.*PC\\.csv$", recursive = FALSE, full.names = TRUE)
tmp_files

# do we actually want to store all of thes efiles here, or just load each adjut then save it back due to memory issues
# try again

# should have removed this before running the code below
# rm(dta_gsr)

# Custom function to read a file and adjust its column names
# Function to read a dataset and adjust column names based on the first two rows
# Overview:
# 1. Reads the first two rows of the file to use as basis for creating unique column names.
# 2. Combines these two rows to form a single set of column names for the dataset.
# 3. Reads the entire dataset, skipping the first two rows which were used for column names.
# 4. Assigns the combined column names to the dataset.
# 5. Adds a column indicating the filename from which the data was read.
# 6. This approach ensures proper data type handling by skipping header rows during the full data read.


# Function to read a dataset and adjust column names based on the first two rows
# Overview:
# 1. Reads the first two rows of the file to use as basis for creating unique column names.
# 2. Combines these two rows to form a single set of column names for the dataset.
# 3. Reads the entire dataset, skipping the first two rows which were used for column names.
# 4. Assigns the combined column names to the dataset.
# 5. Adds a column indicating the filename from which the data was read.
# 6. This approach ensures proper data type handling by skipping header rows during the full data read.

fn_read_and_adjust <- function(file_path) {
  # Read the first two rows for column names adjustment
  header_info <- fread(file_path, nrows = 2, header = FALSE)
  
  # Combine the first two rows to form the column names, enhancing uniqueness
  col_names <- paste0(unlist(header_info[1,]), "_", unlist(header_info[2,]))
  
  # Read the full dataset, explicitly skipping the first two header rows
  data <- fread(file_path, skip = 3, header = FALSE) #using 3 just to be sure
  setnames(data, old = colnames(data), new = col_names) # Assign new column names
  
  # Add a column with the filename to keep track of the data source
  data[, filename := basename(file_path)]
  
  return(data)
}


note this below is not6 quite for ledalab  -see ahead

```{r}
# Predefined column names
final_files_to_load
colnames(dta_gsr[,2:12])
gsr_column_names <- colnames(dta_gsr[,2:12])
gsr_column_names
# 1] "gsr_pulse_timestamp_sync_unix_cal_ms"            
#  [2] "gsr_pulse_battery_cal_m_v"                       
#  [3] "gsr_pulse_gsr_range_cal_no_units"                
#  [4] "gsr_pulse_gsr_skin_conductance_cal_u_s"          
#  [5] "gsr_pulse_gsr_skin_resistance_cal_k_ohms"        
#  [6] "gsr_pulse_ppg_a13_cal_m_v"                       
#  [7] "gsr_pulse_ppg_ibi_cal_ms"                        
#  [8] "gsr_pulse_pp_gto_hr_cal_bpm"                     
#  [9] "gsr_pulse_pressure_bmp280_cal_k_pa"              
# [10] "gsr_pulse_temperature_bmp280_cal_degrees_celsius"
# [11] "gsr_pulse_event_marker_cal_no_units"


# Helper function to extract participant ID and session from filename
extract_info <- function(filename) {
  parts <- str_match(filename, ".*/(\\d+)_Session(\\d+)_GSRPulse_.*\\.csv$")
  return(list(participant_id = parts[, 2], session = parts[, 3]))
}

# Create a dataframe to map participant IDs and sessions to file paths
# flatten(final_files_to_load$file_path)
file_info_2 <- sapply(final_files_to_load$file_path, 
                    function(x) extract_info(x), 
                    simplify = FALSE)

file_df <- do.call(rbind, 
                   lapply(file_info_2, function(x) data.frame(participant_id = 
                                                              x$participant_id, 
                                                            session = x$session, stringsAsFactors = FALSE)))


file_df$filepath <- final_files_to_load$file_path
file_df$file_size <- final_files_to_load$file_size
# Process a list of file paths, adjust their column names, and combine into a single data.table
# Clean column names using `janitor::clean_names` for consistency
# Assuming tmp_files is a list of file paths
# file_df$participant_id = file_df$participant_id 
# participant_id = 136
rm(participant_id)

for (participant_id in unique(file_df$participant_id)) {
  # Find all files for this participant
  participant_files <- file_df[file_df$participant_id == participant_id, ]
  
  # Initialize an empty list to hold session data
  all_sessions_data <- list()
  
  for (i in 1:nrow(participant_files)) {
    
    message("\nProcessing Participant ID: ", participant_id)
    # Load and process each file
    filepath <- participant_files$filepath[i]
    session <- participant_files$session[i]

    # Read the CSV file, skip the first 3 rows
    tmp_gsr <- fread(filepath, skip = 3, header = FALSE)
    
    # Check if the dataframe has 10 columns, and if so, add an empty 11th column
if (ncol(tmp_gsr) == 10) {
  # Add an 11th column filled with NA
  tmp_gsr[, V11 := NA]
}

# Proceed with removing extra columns if necessary
extra_cols <- ncol(tmp_gsr) - length(gsr_column_names)
if (extra_cols > 0) {
  tmp_gsr <- tmp_gsr[, -((ncol(tmp_gsr)-extra_cols+1):ncol(tmp_gsr)), with = FALSE]
}

# Now assign predefined column names, assuming 'gsr_column_names' is of length 11
setnames(tmp_gsr, old = names(tmp_gsr), new = gsr_column_names)


    # Assign predefined column names
    setnames(tmp_gsr, old = names(tmp_gsr), new = gsr_column_names)
    
    # Add a new column with the filename
    filename <- basename(filepath)
    tmp_gsr[, filename := filename]
    
    tmp_gsr[, session := session]
    
    # Correct original timestamp column name to match GSRPulse data
    tmp_gsr[, orig_timestamp_gsr_pulse := gsr_pulse_timestamp_sync_unix_cal_ms]
    
    # Merge timestamps and other operations from dta_psypy1
    tmp_psypy <- dta_psypy1[participant == participant_id, .(participant, social_nonsocial, stim_iaps, trial_no_all, unixtime_start, orig_unixtime_start_psypy)]
    
    tmp_psypy[, next_unixtime_start := shift(unixtime_start, type = "lead", fill = Inf)]
    tmp_psypy[, unixtime_end := next_unixtime_start - 1]
    
    # Initialize missing columns in tmp_gsr
    tmp_gsr[, `:=` (social_nonsocial = as.character(NA), 
                         stim_iaps = as.character(NA), 
                         trial_no_all = as.integer(NA), 
                         psypy_unixtime_start = as.numeric(NA),
                         orig_unixtime_start_psypy = as.numeric(NA))]
    
    # Perform the non-equi join and update
    tmp_gsr[tmp_psypy, on = .(gsr_pulse_timestamp_sync_unix_cal_ms >= unixtime_start, 
                                   gsr_pulse_timestamp_sync_unix_cal_ms <= unixtime_end), 
                 `:=` (social_nonsocial = i.social_nonsocial, 
                       stim_iaps = i.stim_iaps, 
                       trial_no_all = i.trial_no_all, 
                       psypy_unixtime_start = i.unixtime_start,
                       orig_unixtime_start_psypy = i.orig_unixtime_start_psypy), 
                 by = .EACHI]
    
    # Append the processed session data to the list
    all_sessions_data[[length(all_sessions_data) + 1]] <- tmp_gsr
  }
  
  # Combine data from all sessions
  combined_data <- do.call(rbind, all_sessions_data)
  
  # Convert combined data to tibble for dplyr operations
  combined_data <- as_tibble(combined_data)
  
  # Apply dplyr operations
  combined_data <- combined_data %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%
    mutate(start_true = !duplicated(trial_no_all) & !is.na(trial_no_all),
           trigger = case_when(
             trial_no_all == 1 & start_true == TRUE ~ 1,
             start_true == FALSE ~ 0,
             TRUE ~ as.numeric(trial_no_all)),
           gsr_pulse_timerezero_sync_unix_cal_ms = gsr_pulse_timestamp_sync_unix_cal_ms - first(gsr_pulse_timestamp_sync_unix_cal_ms))%>%
    mutate(participant = participant_id)
  
# Just before writing out the combined_data
unique_triggers <- length(unique(combined_data$trigger))
unique_trigger_gt_zero <- combined_data %>% 
  filter(trigger > 0) %>%
  count(trigger) %>%
  filter(n > 1)

# Check for conditions
if (unique_triggers != 66 || nrow(unique_trigger_gt_zero) > 0) {
  warning_message <- paste("Warning: Data integrity checks failed for Participant ID:", participant_id)
  
  if (unique_triggers != 66) {
    warning_message <- paste(warning_message, "\nExpected 66 unique trigger values, but found", unique_triggers)
  }
  
  if (nrow(unique_trigger_gt_zero) > 0) {
    warning_message <- paste(warning_message, "\nFound trigger values greater than 0 that are present more than once:")
    for (i in 1:nrow(unique_trigger_gt_zero)) {
      warning_message <- paste(warning_message, unique_trigger_gt_zero$trigger[i], "(", unique_trigger_gt_zero$n[i], "times);")
    }
  }
  
  # Print the warning message
  message(warning_message)
} else {
  # If all checks pass, proceed with writing the file
  fwrite(as.data.table(combined_data), paste0(participant_id, "_allsessions", "_dta_gsr_pulse_trig.csv"), row.names = FALSE)
  
  # Print completion message for current participant
  message("Completed processing for Participant ID: ", participant_id)
}
}


```
Participant ID: 208
Warning: Data integrity checks failed for Participant ID: 208 
Expected 66 unique trigger values, but found 32

this participant is to be excludethey didn't complete the task
so we should have 69



```{r eval=FALSE, include=FALSE}

# assuming 1.5 hours times 93 peoples recordinds made at 128 hz, how many rows should we expect this data to have

# Given values
# hours_per_recording = 1.5
# seconds_per_hour = 3600
# sampling_rate_hz = 128
# number_of_participants = 93
# 
# # Calculate total seconds per recording
# seconds_per_recording = hours_per_recording * seconds_per_hour
# 
# # Calculate number of samples per recording
# samples_per_recording = seconds_per_recording * sampling_rate_hz
# 
# # Calculate total number of rows for all recordings
# total_rows = samples_per_recording * number_of_participants
# total_rows
# 
# 64281600.0
# less than expected
# nrow(dta_gsr)/64281600.0


# issue here, when using a different sensor, e.g. fd29 the columns are named ioncomnsistently 
# e.g. v3_shimmer_fd29_gsr_range_cal" ratyher than v3_gsr_pulse_gsr_range_cal"  
# so these ietehr need to be renamed manually or programatically
```
combine with timestamps from psychopy





just some checks
```{r}
tmp_gsr
unique(tmp_gsr$trial_no_all)

tmp_gsr<-  read_csv("136_allsessions_dta_gsr_pulse_trig.csv")
View(X136_allsessions_dta_gsr_pulse_trig)

# tmp_gsr
tmp_gsr %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%

    # subset(trial_no_all>10 & trial_no_all<18)%>%
  mutate(time_diff_psypy_gsr = orig_unixtime_start_psypy-orig_timestamp_gsr_pulse)%>%
  ggplot( aes(x = gsr_pulse_timestamp_sync_unix_cal_ms, 
                         y = gsr_pulse_gsr_skin_conductance_cal_u_s)) +
  geom_line() 
 
names(X136_Session1_GSRPulse_Calibrated_PC)<- gsr_column_names

names(X136_Session1_GSRPulse_Calibrated_SD)<- gsr_column_names

tmp_gsr %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%

    # subset(trial_no_all>10 & trial_no_all<18)%>%
  # mutate(time_diff_psypy_gsr = orig_unixtime_start_psypy-orig_timestamp_gsr_pulse)%>%
  ggplot( aes(x = gsr_pulse_timestamp_sync_unix_cal_ms, 
                         y = gsr_pulse_gsr_skin_conductance_cal_u_s)) +
  geom_line() 


X136_Session1_GSRPulse_Calibrated_PC %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%

    # subset(trial_no_all>10 & trial_no_all<18)%>%
  # mutate(time_diff_psypy_gsr = orig_unixtime_start_psypy-orig_timestamp_gsr_pulse)%>%
  ggplot( aes(x = gsr_pulse_timestamp_sync_unix_cal_ms, 
                         y = gsr_pulse_gsr_skin_conductance_cal_u_s)) +
  geom_line() 

X136_Session1_GSRPulse_Calibrated_SD %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%

    # subset(trial_no_all>10 & trial_no_all<18)%>%
  # mutate(time_diff_psypy_gsr = orig_unixtime_start_psypy-orig_timestamp_gsr_pulse)%>%
  ggplot( aes(x = gsr_pulse_timestamp_sync_unix_cal_ms, 
                         y = gsr_pulse_gsr_skin_conductance_cal_u_s)) +
  geom_line() 
 
  
  
  # tmp_merged%>%
  #     # subset(trial_no_all>10 & trial_no_all<18)%>%
  # mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-unixtime_start)%>%
  # # group_by()
  # # mutate(timenew = orig_timestamp_gsr[trigger!= 0])%>%
  # ggplot(aes(orig_timestamp_gsr, gsr_pulse_gsr_skin_conductance_cal_u_s))+
  # geom_line()


tmp_gsr

tmp_merged_test<- tmp_merged%>%
  subset(!duplicated(trigger))


tmp_merged_test$orig_timestamp_gsr[tmp_merged_test$trigger >0]-
tmp_gsr$gsr_pulse_timestamp_sync_unix_cal_ms[tmp_gsr$trigger >0]

# [1]  0.0046386719 -0.0004882812  0.0024414062  0.0036621094 -0.0029296875  0.0021972656  0.0019531250  0.0021972656 -0.0009765625
# [10]  0.0043945312  0.0014648438 -0.0021972656  0.0000000000  0.0026855469  0.0000000000  0.0029296875  0.0004882812 -0.0034179688
# [19]  0.0039062500  0.0004882812  0.0024414062 -0.0012207031 -0.0017089844 -0.0041503906 -0.0039062500  0.0036621094  0.0043945312
# [28] -0.0043945312 -0.0046386719  0.0031738281 -0.0031738281 -0.0048828125 -0.0029296875 -0.0041503906  0.0009765625  0.0029296875
# [37]  0.0048828125 -0.0051269531 -0.0043945312  0.0024414062 -0.0024414062 -0.0014648438 -0.0002441406 -0.0009765625 -0.0014648438
# [46] -0.0019531250  0.0039062500 -0.0012207031 -0.0002441406 -0.0012207031 -0.0029296875 -0.0014648438  0.0014648438  0.0017089844
# [55]  0.0021972656  0.0031738281  0.0007324219 -0.0007324219 -0.0036621094  0.0007324219 -0.0019531250  0.0017089844  0.0000000000
# [64]  0.0041503906  0.0041503906
# expected
```

this actually works and agrees with the previopusnone
just with this for now, but we need to investigate wether this is happening slighlly after we intend it




write to ledalab

# Data Preparation for Ledalab Analysis
# Note: 
# - Ledalab has very specific format requirements that cna create issues= needs .txt without column names.
# - The presence of 'NA' values, weird characters, or numbers as characters (e.g "1") even empty spaces can cause loading, opening, or importing issues within Ledalab. It may lead to repeated prompts for specifying variables, which indicates these common problems.
# - It is crucial to meticulously prepare and clean the data to meet these specific requirements to avoid any operational disruptions in Ledalab.

# Comparative Validation Note:
# - A manual examination was conducted between a dataset processed mkanually (participant 110) and another processed through this automated script to gtenearte trigger. This was done to ensure that the start times, particularly for GSR (Galvanic Skin Response) data, accurately match when analyzed in Ledalab.
# - This comparison confirmed that both the timing and the pattern of analysis remain consistent between manually processed and automated datasets. 


for ledalab, we'll just write time stamp (original and re zeroed, skin conductance data) and triggers, anyt medatdasta wil be matched later - based on trigger (consider writing ann aggreate macthign file from trigger to psypy?)

this one below export the fulld ataset, but its best to export rest and trials separatelly
```{r eval=FALSE, include=FALSE}

colnames(tmp_gsr)

tmp_gsr$time

# Define the directory where participant CSV files are stored
setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/GSR/gsr_wtriggers/")

input_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/GSR/gsr_wtriggers/"

# Define the output directory for the formatted files
output_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/GSR/gsr_wtriggers/format4leda/all unsplit/"

library(dplyr)

# List all CSV files in the directory
participant_files <- list.files(input_directory, pattern = "\\.csv$", full.names = TRUE)

# Function to process each file
process_participant_file <- function(file_path) {
  # Read the CSV file
  data <- read.csv(file_path)%>%
    select(gsr_pulse_timerezero_sync_unix_cal_ms,
           gsr_pulse_timestamp_sync_unix_cal_ms,
           gsr_pulse_gsr_skin_conductance_cal_u_s,
           trigger)
  
  
  # Arrange by unixtime_start if needed
  selected_data <- data %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms)
  
  # Construct output file name based on input file name and output directory
 
 # Remove the file extension and append "_ledaform.txt" to construct the output file name
  file_name_no_ext <- tools::file_path_sans_ext(basename(file_path))
  output_file_name <- paste0(output_directory, "/", file_name_no_ext, "_ledaform.txt")
  
  
  # Write the selected and potentially arranged data to a new file, format for Ledalab
  write.table(selected_data, 
              output_file_name, sep = "\t",
              row.names = FALSE, 
              col.names = FALSE)
  # Print a message indicating progress
  cat("Processed and written:", output_file_name, "\n")
}

# Apply the function to each file
lapply(participant_files, process_participant_file)
```


export trials and rest period separatelly
```{r}

input_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/GSR/gsr_wtriggers/"

# Define the output directory for the formatted files
output_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/GSR/gsr_wtriggers/format4leda/"
# Assuming input_directory and output_directory are defined
participant_files <- list.files(input_directory, pattern = "\\.csv$", full.names = TRUE)


process_participant_file <- function(file_path) {
  # Read the CSV file and select relevant columns
  data <- read.csv(file_path) %>%
    select(gsr_pulse_timerezero_sync_unix_cal_ms,
           gsr_pulse_timestamp_sync_unix_cal_ms,
           gsr_pulse_gsr_skin_conductance_cal_u_s,
           trigger) %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms)

  # Find indices for trigger conditions
  first_trigger_2_index <- which(data$trigger == 2)[1]
  first_trigger_1_index <- which(data$trigger == 1)[1]

  # Function to rezero time
  rezero_time <- function(df) {
    df %>%
      mutate(gsr_pulse_timerezero_sync_unix_cal_ms = gsr_pulse_timerezero_sync_unix_cal_ms -
               gsr_pulse_timerezero_sync_unix_cal_ms[1])
  }

  # Data before first occurrence of trigger == 2
  if (!is.na(first_trigger_2_index) && first_trigger_2_index > 0) {
    data_before_trigger_2 <- data %>%
      slice(1:(first_trigger_2_index - 1)) %>%
      rezero_time()

    # Construct output file name for data before trigger == 2
    file_name_no_ext <- tools::file_path_sans_ext(basename(file_path))
    output_file_name_before_trigger_2 <- paste0(output_directory, "/", file_name_no_ext, "_rest_ledaform.txt")

    # Write to file
    write.table(data_before_trigger_2, output_file_name_before_trigger_2, sep = "\t", row.names = FALSE, col.names = FALSE)
    cat("Processed and written (before trigger 2):", output_file_name_before_trigger_2, "\n")
  }

  # Data after first occurrence of trigger == 1, excluding the trigger row itself
  if (!is.na(first_trigger_1_index) && first_trigger_1_index > 0) {
    data_after_trigger_1 <- data %>%
      slice((first_trigger_1_index + 1):n()) %>%
      rezero_time()

    # Construct output file name for data after trigger == 1
    output_file_name_after_trigger_1 <- paste0(output_directory, "/", file_name_no_ext, "_pract_nd_trials_ledaform.txt")

    # Write to file
    write.table(data_after_trigger_1, output_file_name_after_trigger_1, sep = "\t", row.names = FALSE, col.names = FALSE)
    cat("Processed and written (after trigger 1):", output_file_name_after_trigger_1, "\n")
  }
}


# Process each file
lapply(participant_files, process_participant_file)

```


# ==============================================================================
# README: Preparation and Analysis of Ledalab Formatted Files
# ==============================================================================
ledaform files
GSR files were processed to select relevant columns and arrange data by timestamp, ensuring compatibility with Ledalab software. Output files are saved without column names, using tab-separated values (TSV), in line with Ledalab's input requirements. note ledalab has an issue wityh NAs, strringds, empty columsn or even numnumebrsebrs wrriten like "11".
any issues with loading likely has to do with that

### Output File Format:
Each file is named after the original participant data file, appended with "_ledaform.txt" to indicate formatting for Ledalab. The files include the following columns:
  1. **gsr_pulse_timerezero_sync_unix_cal_ms**: Timestamp (Unix time) after rezeroing, indicating synchronized GSR pulse time.
  2. **gsr_pulse_timestamp_sync_unix_cal_ms**: Original Unix timestamp of the GSR pulse.
  3. **gsr_pulse_gsr_skin_conductance_cal_u_s**: Calibrated skin conductance level in microsiemens (μS).
  4. **trigger**: Event marker or trigger signal associated with the measurement.

### Sampling Rate and Downsampling:
- **Original Sampling Rate**: 128Hz.
- **Downsampling Target**: 16Hz (or the lowest rate that maintains a frequency above 10Hz), corresponding to a downsampling factor of 8 (128Hz / 8 = 16Hz).
- **Method**: Use "factor mean" in Ledalab for downsampling, averaging data points within each factor to preserve signal characteristics.

## Importing Data into Ledalab:
Select 'Data Type 3' (manual definition) upon import to manually specify data structure for accurate interpretation and analysis within Ledalab.

# Analysis Guidelines for Ledalab:

## Preliminary Manual Analysis (GUI):
Before batch processing, it's essential to manually analyze a one or two files to establish a benchmark for comparison and ensure the analysis settings are correctly configured. This will also serve as a referecne for automated analysis

### Steps for Manual Analysis:
1. **Open Ledalab's GUI**: Load a files from 'rest' and 'pract_nd_trial' categories to familiarize with the data and settings.
2. **Configure Analysis Settings**:
   - Enable **Adaptive Smoothing**: Adjusts the smoothing parameter dynamically for each data segment to improve the signal-to-noise ratio.

3. **Run CDA Analysis**: Perform Continuous Decomposition Analysis to decompose the GSR signal into its tonic and phasic components.
   - **Optimize Twice**: Enhances the accuracy of the Continuous Decomposition Analysis (CDA) by refining the decomposition parameters iteratively.
4. **Export event related activation**:
   - For trials: Analyze event-related data within a window of **1 to 7 seconds** (or **0.5 to 7 seconds** if permissible).
   - For rest: Use a window of **1 to 120 seconds** to capture the full range of resting data.
5. **Export and Save Results**: Ensure to save `.mat` files for the analysis results and key plots for later comparison
also save a mat file of the whole analysis

## Automated Batch Processing:
After manual analysis, automate the processing for all files using Ledalab's batch mode. This ensures consistent application of the predetermined settings across all files.

### Batch Processing Steps:
- Visit [Ledalab Batch Mode Documentation](http://www.ledalab.de/documentation.htm#batchmode) for detailed instructions on setting up batch processing.

#### Sample Batch Processing Commands: check the slashes as t5hey differ on windows and mac

- **For Practice and Trials**:
Ledalab('D:\OneDrive - Nexus365\InteroStudy2020\analysis\DataAnalysisJanuary2020\DataAnalysisJan2020\GP_gsr_forLeda', 'open','text3','downsample', 15, 'smooth',{'adapt'}, 'analyze','CDA', 'optimize',3, 'export_era', [1 7 .01 2],  'overview',1)

Code for automatic rest analysis
Ledalab('D:\OneDrive - Nexus365\InteroStudy2020\analysis\DataAnalysisJanuary2020\DataAnalysisJan2020\GP_gsr_forLeda', 'open','text3','downsample', 15, 'smooth',{'adapt'}, 'analyze','CDA', 'optimize',3, 'export_era', [1 120 .01 2],  'overview',1)


insopect some fo teh saved ouputs and plots and liusyt of problem files any files you can't run in batch mode run manualy


combine the ledalab output with behavioural data and check if we have what we would expect

3. Plot Physio data in R and Ledalab



Pulse rate

```{r}
colnames(tmp_gsr)




library(readr)

tmp_gsr <- read_csv("225_allsessions_dta_gsr_pulse_trig.csv")
table(tmp_gsr$gsr_pulse_ppg_ibi_cal_ms == -1)
unique(tmp_gsr$trial_no_all)

test<- tmp_gsr%>%
  subset(gsr_pulse_ppg_ibi_cal_ms!=-1)

unique(test$stim_iaps)

tmp_gsr%>%
  ggplot(aes(gsr_pulse_timerezero_sync_unix_cal_ms,
             gsr_pulse_pp_gto_hr_cal_bpm))+
  geom_line()


tmp_gsr%>%
  subset(gsr_pulse_ppg_ibi_cal_ms!= -1)%>%
  ggplot(aes(gsr_pulse_timerezero_sync_unix_cal_ms,
             gsr_pulse_ppg_ibi_cal_ms))+
  geom_line()


colnames(tmp_gsr)
tmp_gsr%>%
  # subset(gsr_pulse_ppg_ibi_cal_ms!= -1)%>%
  ggplot(aes(gsr_pulse_timerezero_sync_unix_cal_ms,
             gsr_pulse_ppg_a13_cal_m_v))+
  geom_line()
  xlim(0,10000)
  
  
  
  tmp_gsr%>%
  subset(gsr_pulse_ppg_ibi_cal_ms!= -1)%>%
    subset(gsr_pulse_ppg_ibi_cal_ms>400)%>%
    mutate(ibi_lead = lead(gsr_pulse_ppg_ibi_cal_ms))%>%
  ggplot(aes(gsr_pulse_ppg_ibi_cal_ms,
             ibi_lead))+
  geom_point()
  
  
  # ibi normal range
  # 600 and 900 milliseconds
    tmp_gsr%>%
  subset(gsr_pulse_ppg_ibi_cal_ms!= -1)%>%
      
    mutate(ibi_lead = lead(gsr_pulse_ppg_ibi_cal_ms))%>%
  ggplot(aes(gsr_pulse_timerezero_sync_unix_cal_ms,gsr_pulse_ppg_ibi_cal_ms))+
  geom_point(size = .1)+
      geom_line(size = .1)+
      xlim(1500000, 2000000)
    
   
    
```


```{r}
# Define the maximum number of consecutive NAs to interpolate
max_consecutive_nas <- 3

# Function to limit interpolation to a maximum number of consecutive NAs
# Function to perform cubic spline interpolation with a maximum limit on consecutive NAs
interpolate_limited_na <- function(x, max_consec_na) {
  # Use na.locf to fill in leading and trailing NAs for spline interpolation
  x_filled <- na.locf(x, fromLast = TRUE)
  x_filled <- na.locf(x_filled)
  # Apply cubic spline interpolation
  x_spline <- na.spline(x_filled)
  # Replace the values that were originally NAs and exceed consecutive NA limit
  na_runs <- rle(is.na(x))
  too_long <- which(na_runs$lengths > max_consec_na & na_runs$values)
  for (i in too_long) {
    start_index <- sum(na_runs$lengths[1:(i-1)]) + 1
    end_index <- sum(na_runs$lengths[1:i])
    x_spline[start_index:end_index] <- x[start_index:end_index]
  }
  return(x_spline)
}


# Function to identify artifacts based on the first and fourth quartiles of the absolute differences
ibi_artifacts_quartiles <- function(ibi_values) {
  ibi_diff <- abs(c(NA, diff(ibi_values))) # Calculate absolute differences, NA for the first undefined difference
  q1 <- quantile(ibi_diff, 0.30, na.rm = TRUE) # First quartile
  q3 <- quantile(ibi_diff, 0.70, na.rm = TRUE) # Fourth quartile
  # Identify indices where the difference is outside the quartiles
  artifact_indices <- which(ibi_diff < q1 | ibi_diff > q3)
  ibi_values[artifact_indices] <- NA
  return(ibi_values)
}

# IBI



# Define the directory where your tmp_gsr files are stored
file_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/GSR/gsr_wtriggers"

# List all CSV files in the directory
files <- list.files(file_directory, pattern = "\\.csv$", full.names = TRUE)


# Process and write each file
lapply(files, function(file_path) {
  # Load the file
  tmp_pulse <- read_csv(file_path)%>%
    select(filename, participant, trigger, gsr_pulse_timestamp_sync_unix_cal_ms, social_nonsocial, stim_iaps, trial_no_all, gsr_pulse_ppg_ibi_cal_ms)
  
  # Apply your data manipulation pipeline
  processed_data <- tmp_pulse %>%
    
    # isolate and clean IBI
    filter(gsr_pulse_ppg_ibi_cal_ms != -1) %>%
    mutate(gsr_pulse_ppg_ibi_cal_ms_cleaned = ibi_artifacts_quartiles(gsr_pulse_ppg_ibi_cal_ms)) %>%
    mutate(gsr_pulse_ppg_ibi_cal_ms_interpolated = interpolate_limited_na(gsr_pulse_ppg_ibi_cal_ms_cleaned, max_consecutive_nas)) %>%
    
    # keep just  data window trial windows
    filter(!is.na(trial_no_all)) %>%
    group_by(trial_no_all) %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%
    mutate(time_end = if_else(!duplicated(trial_no_all), gsr_pulse_timestamp_sync_unix_cal_ms + 7000, NA_real_)) %>%
    mutate(time_end = if_else(trial_no_all == 1, gsr_pulse_timestamp_sync_unix_cal_ms + 120000, time_end)) %>%
    fill(time_end, .direction = "down") %>%
    filter(gsr_pulse_timestamp_sync_unix_cal_ms <= time_end) %>%
    
    ungroup() %>%
    # select variables to keep
    select(filename, participant, gsr_pulse_timestamp_sync_unix_cal_ms, social_nonsocial, stim_iaps, trial_no_all, gsr_pulse_ppg_ibi_cal_ms, gsr_pulse_ppg_ibi_cal_ms_cleaned, gsr_pulse_ppg_ibi_cal_ms_interpolated) %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%
    group_by(trial_no_all) %>%
    mutate(pulse_ibi_percent_valid = mean(!is.na(gsr_pulse_ppg_ibi_cal_ms_interpolated)) * 100) %>%
    ungroup()

  # Construct the output file path
  output_file_path <- gsub("path/to/your/files", "path/to/your/output/directory", file_path)
  ppt_1<- unqiue(processed_data$participant)
  # Write the processed data to a new CSV file
  write_csv(processed_data, paste0(ppt_1, "pulse_ibi.csv"))
})


```

delete below
# Pre-process gsr_pulse_ppg_ibi_cal_ms to handle NA values and interpolate
 tmp_gsr %>%
   subset(gsr_pulse_ppg_ibi_cal_ms!= -1)%>%
  # mutate(gsr_pulse_ppg_ibi_cal_ms_na = if_else(gsr_pulse_ppg_ibi_cal_ms > 0, gsr_pulse_ppg_ibi_cal_ms, NA_real_)) %>%
  mutate(gsr_pulse_ppg_ibi_cal_ms_cleaned = 
           ibi_artifacts_quartiles(gsr_pulse_ppg_ibi_cal_ms)) %>%
  mutate(gsr_pulse_ppg_ibi_cal_ms_interpolated = interpolate_limited_na(gsr_pulse_ppg_ibi_cal_ms_cleaned, max_consecutive_nas))%>%

# Main analysis pipeline

  # Filter out trials without valid trial numbers early to simplify downstream analysis
  filter(!is.na(trial_no_all)) %>%
  
  group_by(trial_no_all) %>%
   arrange(gsr_pulse_timestamp_sync_unix_cal_ms)%>%
  
  # Calculate time_end based on conditions, filling forward within each group
  mutate(time_end = if_else(!duplicated(trial_no_all), gsr_pulse_timestamp_sync_unix_cal_ms + 7000, NA_real_)) %>%
  mutate(time_end = if_else(trial_no_all == 1, gsr_pulse_timestamp_sync_unix_cal_ms + 120000, time_end)) %>%
  fill(time_end, .direction = "down") %>%
  
  # Exclude data outside of valid time_end ranges
  filter(gsr_pulse_timestamp_sync_unix_cal_ms <= time_end) %>%
  
  ungroup() %>%
  
  # Select relevant columns for output, ensuring a clean dataset for analysis
  select(filename,
         participant,
         gsr_pulse_timestamp_sync_unix_cal_ms,
         social_nonsocial,
         stim_iaps,
         trial_no_all,
         gsr_pulse_ppg_ibi_cal_ms,
         # gsr_pulse_ppg_ibi_cal_ms_na,
         gsr_pulse_ppg_ibi_cal_ms_cleaned,
         gsr_pulse_ppg_ibi_cal_ms_interpolated) %>%
  
  # Arrange data by timestamp to ensure chronological order
  arrange(gsr_pulse_timestamp_sync_unix_cal_ms)%>%
  group_by(trial_no_all)%>%
   mutate(pulse_ibi_percent_valid = mean(!is.na(gsr_pulse_ppg_ibi_cal_ms_interpolated)) * 100)

test%>%
  ggplot(aes(pulse_ibi_percent_valid))+
  geom_histogram()
# then load and all sumamrise and average
  group_by(trial_no_all)%>%
  summarise_if(is.numeric, mean, na.rm = T)
  mutate(ibi_percent_valid)

# Use the processed_gsr for further analysis
unique(test$trial_no_all)
```



# table(tmp_gsr$gsr_pulse_ppg_ibi_cal_ms == -1)

unique(tmp_gsr$stim_iaps)
unique(tmp_gsr$stim_iaps)

table(tmp_gsr$gsr_pulse_ppg_ibi_cal_ms> 0)
unique(tmp_gsr$trial_no_all)
unique(test$trial_no_all)

test<- tmp_gsr %>%
  group_by(trial_no_all)%>%
  mutate(time_end = if_else(start_true == TRUE, 
                            gsr_pulse_timestamp_sync_unix_cal_ms + 7000, NA)) %>%
  mutate(time_end = if_else(trigger == 1, 
                            gsr_pulse_timestamp_sync_unix_cal_ms + 120000, time_end)) %>%
  
   # Fill down NA values in time_end within each group; replace with the nearest non-NA value above
  fill(time_end, .direction = "down") %>%
   # subset(trigger>0)%>%
  mutate(gsr_pulse_ppg_ibi_cal_ms_na = if_else(gsr_pulse_ppg_ibi_cal_ms >0, gsr_pulse_ppg_ibi_cal_ms, NA))%>%
  
  ungroup()%>%
  mutate(gsr_pulse_ppg_ibi_cal_ms_cleaned = ibi_artifacts_quartiles(gsr_pulse_ppg_ibi_cal_ms_na)) %>%
  mutate(gsr_pulse_ppg_ibi_cal_ms_interpolated = interpolate_limited_na(gsr_pulse_ppg_ibi_cal_ms_cleaned, max_consecutive_nas))%>%

# now exclude periods before ansd after trial
 filter(!is.na(trial_no_all))%>%
    group_by(trial_no_all)%>%
  filter(gsr_pulse_timestamp_sync_unix_cal_ms <= time_end)%>%
  arrange(gsr_pulse_timestamp_sync_unix_cal_ms)%>%
  select(filename, gsr_pulse_timestamp_sync_unix_cal_ms,
         social_nonsocial,
         stim_iaps,
         trial_no_all,
         # trigger,
         gsr_pulse_ppg_ibi_cal_ms,
         gsr_pulse_ppg_ibi_cal_ms_na,
         gsr_pulse_ppg_ibi_cal_ms_cleaned,
         
         gsr_pulse_ppg_ibi_cal_ms_interpolated
         )

         
         
   unique(test$trial_no_all)      
         
  

  

```

# heart rate
colnames(tmp_gsr)


# Decide on max consecutive NAs to fill
max_consec_nas_to_fill_hr <- 128

# Function to limit filling to gaps of a certain size
limited_fill_hr <- function(x, max_gap) {
  na_runs <- rle(is.na(x))
  too_long <- which(na_runs$lengths > max_gap & na_runs$values)
  for (i in too_long) {
    indices <- sum(na_runs$lengths[1:(i-1)]) + (1:na_runs$lengths[i])
    x[indices] <- NA  # Reset these values to NA
  }
  return(na.approx(x, na.rm = FALSE))
}

range(tmp_gsr)

tmp_gsr %>%
  # subset(gsr_pulse_ppg_ibi_cal_ms!=-1)%>%
  mutate(gsr_pulse_pp_gto_hr_cal_bpm_na = if_else(gsr_pulse_pp_gto_hr_cal_bpm > 35 &
                                                    gsr_pulse_pp_gto_hr_cal_bpm < 200, 
                                                  gsr_pulse_pp_gto_hr_cal_bpm, NA))%>%
  mutate(hr_zscore = scale(gsr_pulse_pp_gto_hr_cal_bpm_na))%>%
  
mutate(gsr_pulse_pp_gto_hr_cal_bpm_na = if_else(abs(hr_zscore)<3,gsr_pulse_pp_gto_hr_cal_bpm_na, NA))%>%
mutate(gsr_pulse_pp_gto_hr_cal_bpm_interp = limited_fill_hr(gsr_pulse_pp_gto_hr_cal_bpm_na, 128))%>%           
  ggplot(aes(gsr_pulse_timerezero_sync_unix_cal_ms,  gsr_pulse_pp_gto_hr_cal_bpm_interp))+
  geom_point(size = .1)+
  geom_line(size = .1)+
   # geom_line(aes(y = gsr_pulse_pp_gto_hr_cal_bpm), size = .1, colour = "blue")+
  geom_line(aes(y = gsr_pulse_pp_gto_hr_cal_bpm_na), size = .1, colour = "pink")+
  xlim(5000, 730000)


```

