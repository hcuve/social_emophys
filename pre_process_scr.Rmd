---
title: "pre_process SCR shimmer data - social emophys"
author: "Helio"
date: "2024-03-07"
output: html_document
---



2. Export/create triggers from behavioural data (merge physio with psychopy)

COMBINE TRIGGERS WITH PHYSIO


Set-up: where I load libraries and create objects for later use (such as color schemes etc).
```{r setup, include=FALSE}
# Load necessary libraries
require(tidyverse)
require(data.table)
library(janitor)
library(purrr)

# Define paths
data_path <- "./Data backup/DataChecks/R code and data/feb 6"
ctrl_data_path <- file.path(data_path, "ctrl")


```


load files 
```{r}
# data_psypy1

base_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio" # Change this to your actual base directory path


# List immediate subdirectories first (non-recursive)
# subdirs <- list.dirs(path = base_directory, recursive = FALSE, full.names = TRUE)

# Initialize a vector to hold directories that match the "Export" pattern
export_dirs <- c()

# Now, for each immediate subdir, look one level down for "Export" folders
for (dir in subdirs) {
  cat("Checking in:", dir, "\n") # Simple progress feedback
  # Assuming "Export" directories are directly under these subdirs
  potential_exports <- list.dirs(path = dir, recursive = FALSE, full.names = TRUE)
  matched_exports <- grep("Consensys", potential_exports, value = TRUE)
  export_dirs <- c(export_dirs, matched_exports)
}

# export_dirs now contains paths to directories matching the "Export" pattern
# Find all sub directories named "raw" within the base directory

export_dirs

# export_dir <- list.files(path = base_directory, 
#                               pattern = "Export", 
#                               full.names = TRUE, 
#                               recursive = TRUE, 
#                               include.dirs = TRUE)


# Custom function to read a file and adjust its column names
fn_read_and_adjust <- function(file_path) {
  # Read the first two rows to adjust column names
  header_info <- fread(file_path, nrows = 1, skip = 1, header = FALSE)
  col_names <- colnames(fread(file_path, nrows = 1, skip = 2, header = FALSE))
  
  # Modify column names based on the header info
  new_col_names <- paste0(col_names, "_", unlist(header_info[1,]))
  
  # Now read the full data (skipping the first two rows) and assign new column names
  data <- fread(file_path, skip = 2, header = FALSE)
  setnames(data, old = colnames(data), new = new_col_names)
  
  # Return the modified data.table
  return(data)
}



```

recursive folder search
```{r}
base_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio"

# Ensure this directory exists and R can access it. If there are access issues, consider absolute paths or check permissions.
# List immediate subdirectories first (non-recursive)
subdirs <- list.dirs(path = base_directory, recursive = FALSE, full.names = TRUE)


# Initialize an empty vector to store the paths of all desired CSV files
csv_files_paths <- c()

# For each "Export" directory, look one level deeper
for (export_dir in export_dirs) {
    # List subdirectories within the current "Export" directory
    subdirs_in_export <- list.dirs(path = export_dir, recursive = FALSE, full.names = TRUE)
    
    # Filter out directories that contain "uncalibrated" in their name
    filtered_subdirs <- grep("uncalibrated", subdirs_in_export, value = TRUE, invert = TRUE, ignore.case = TRUE)
    
    # For each filtered subdirectory, list CSV files
    for (subdir in filtered_subdirs) {
        # List all CSV files in the subdirectory
        csv_files_in_subdir <- list.files(path = subdir, pattern = "\\.csv$", full.names = TRUE)
        
        # Add the found files to the collection
        csv_files_paths <- c(csv_files_paths, csv_files_in_subdir)
    }
}

# Proceed with applying the custom function to the list of CSV files

# Apply the custom function to each CSV file and combine the results

# grep("_PC",csv_files_paths)
# 
# tmp_test <-csv_files_paths[grep("_PC",csv_files_paths)]
# 
# tmp_test[grep("GSR",tmp_test)]

# combined_data <- rbindlist(lapply(csv_files_paths, fn_read_and_adjust), use.names = TRUE, fill = TRUE, idcol = "filename") %>%
#   janitor::clean_names()

# 'combined_data' now contains your adjusted and combined dataset



# why is 217 000



```


simpler approach 
- search on mac and copy easily to a folder called calibrated
- note check logbook for notes on data issues


GSR data should be sampled at 128 hz 

# 1000 ms = 128
# 6000 ms = 128*6 = 768
```{r}

setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated")
# List of .csv files, assuming you've already created 'tmp_files' with list.files()
# tmp_files <- list.files(pattern = "\\._PCcsv$", recursive = FALSE, full.names = TRUE)
tmp_files <- list.files(pattern = "GSR.*PC\\.csv$", recursive = FALSE, full.names = TRUE)

tmp_files_SD <- list.files(pattern = "GSR.*SD\\.csv$", recursive = FALSE, full.names = TRUE)

tmp_files
tmp_files_SD

rm(fn_extract_identifiers)

# Extract identifiers

# Regular expression to capture the pattern before "_" and after "./"
pattern <- "\\./([0-9]+)_.*"

# Extract the pattern
identifiers_tmp_files_PC <- sub(pattern, "\\1",  tmp_files)
identifiers_tmp_files_SD <- sub(pattern, "\\1", tmp_files_SD)

# Compare and find common identifiers
common_identifiers <- intersect(identifiers_tmp_files_PC, identifiers_tmp_files_SD)

# Display the common identifiers
common_identifiers

# Identifiers unique to PC files
unique_identifiers_PC <- setdiff(identifiers_tmp_files_PC, identifiers_tmp_files_SD)

# Identifiers unique to SD files
unique_identifiers_SD <- setdiff(identifiers_tmp_files_SD, identifiers_tmp_files_PC)

# Display the unique identifiers
unique_identifiers_PC
unique_identifiers_SD

# [1] "101" "102" "103" "106"
# > unique_identifiers_SD
# [1] "105"
# copy and rename 105 sd to ad _PC, so we can read all pcs again

# then use this info to load PC files and just add sd wheere needed

```

load gsr data
```{r}

setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated")
###################################################
# Handling file imports from PC and SD sources:
# - This process primarily relies on PC files, but switches to SD files in the absence of a corresponding PC file.
# - A notable discrepancy has been observed in the export formats: PC exports contain "sync" within the timestamps, unlike SD exports. This difference may affect how filenames are generated and could imply variations in how data synchronization is handled or represented between the two sources.
# - It is essential to consider this variation during data processing to ensure consistent naming conventions and to investigate the synchronization status of SD data.
# - The custom function is applied to each file to adjust for these discrepancies, and the results are subsequently combined.

###################################################

tmp_files <- list.files(pattern = "GSR.*PC\\.csv$", recursive = FALSE, full.names = TRUE)
tmp_files

# do we actually want to store all of thes efiles here, or just load each adjut then save it back due to memory issues
# try again

# should have removed this before running the code below
rm(dta_gsr)

# Custom function to read a file and adjust its column names
# Function to read a dataset and adjust column names based on the first two rows
# Overview:
# 1. Reads the first two rows of the file to use as basis for creating unique column names.
# 2. Combines these two rows to form a single set of column names for the dataset.
# 3. Reads the entire dataset, skipping the first two rows which were used for column names.
# 4. Assigns the combined column names to the dataset.
# 5. Adds a column indicating the filename from which the data was read.
# 6. This approach ensures proper data type handling by skipping header rows during the full data read.


# Function to read a dataset and adjust column names based on the first two rows
# Overview:
# 1. Reads the first two rows of the file to use as basis for creating unique column names.
# 2. Combines these two rows to form a single set of column names for the dataset.
# 3. Reads the entire dataset, skipping the first two rows which were used for column names.
# 4. Assigns the combined column names to the dataset.
# 5. Adds a column indicating the filename from which the data was read.
# 6. This approach ensures proper data type handling by skipping header rows during the full data read.
fn_read_and_adjust <- function(file_path) {
  # Read the first two rows for column names adjustment
  header_info <- fread(file_path, nrows = 2, header = FALSE)
  
  # Combine the first two rows to form the column names, enhancing uniqueness
  col_names <- paste0(unlist(header_info[1,]), "_", unlist(header_info[2,]))
  
  # Read the full dataset, explicitly skipping the first two header rows
  data <- fread(file_path, skip = 3, header = FALSE) #using 3 just to be sure
  setnames(data, old = colnames(data), new = col_names) # Assign new column names
  
  # Add a column with the filename to keep track of the data source
  data[, filename := basename(file_path)]
  
  return(data)
}


# Process a list of file paths, adjust their column names, and combine into a single data.table
# Clean column names using `janitor::clean_names` for consistency
# Assuming tmp_files is a list of file paths
tmp_files

dta_gsr <- rbindlist(lapply(tmp_files, fn_read_and_adjust), use.names = TRUE, fill = TRUE) %>%
  janitor::clean_names()%>%
  select(filename, matches("gsr"))


colnames(dta_gsr)
unique(dta_gsr$filename)

head(dta_gsr)
dta_gsr$filename

# taskes a bit of time
dta_gsr<- dta_gsr%>%
  mutate(participant = sub("_.*", "", filename),
         session = gsub(".*_(Session\\d+)_.*", "\\1", filename))



# Replace '000' with '217' in the participant column
# 000 is 217 coded by mistake by the students
dta_gsr$participant <- ifelse(dta_gsr$participant == '000', '217', dta_gsr$participant)

unique(dta_gsr$participant)
unique(dta_gsr$session)



# check nas
dta_gsr %>%
  # Group data by 'participant'
  group_by(participant) %>%
  # Summarise the data to count NA and non-NA values
  summarise(
    num_NA = sum(is.na(gsr_pulse_ppg_a13_cal_m_v)), # Count of NAs
    num_non_NA = sum(!is.na(gsr_pulse_ppg_a13_cal_m_v)) # Count of non-NAs
  )

# test<- dta_gsr%>%subset(participant == 226)## this had NAS
# 
# [1] "217" "101" "102" "103" "104" "105" "106" "107" "108" "109" "110" "111" "112" "113"
# [15] "114" "115" "116" "117" "118" "120" "121" "122" "123" "124" "125" "126" "127" "128"
# [29] "129" "130" "131" "132" "133" "134" "135" "136" "201" "202" "203" "204" "205" "206"
# [43] "207" "208" "209" "210" "211" "212" "213" "214" "215" "216" "218" "219" "220" "221"
# [57] "222" "224" "225" "226" "227" "228" "229" "230" "231" "232" "233" "234"

```


# assuming 1.5 hours times 93 peoples recordinds made at 128 hz, how many rows should we expect this data to have

# Given values
# hours_per_recording = 1.5
# seconds_per_hour = 3600
# sampling_rate_hz = 128
# number_of_participants = 93
# 
# # Calculate total seconds per recording
# seconds_per_recording = hours_per_recording * seconds_per_hour
# 
# # Calculate number of samples per recording
# samples_per_recording = seconds_per_recording * sampling_rate_hz
# 
# # Calculate total number of rows for all recordings
# total_rows = samples_per_recording * number_of_participants
# total_rows
# 
# 64281600.0
# less than expected
# nrow(dta_gsr)/64281600.0


# issue here, when using a different sensor, e.g. fd29 the columns are named ioncomnsistently 
# e.g. v3_shimmer_fd29_gsr_range_cal" ratyher than v3_gsr_pulse_gsr_range_cal"  
# so these ietehr need to be renamed manually or programatically

combine with timestamps from psychopy

```{r eval=FALSE, include=FALSE}
# keep some original time

dta_gsr$orig_timestamp_gsr<- dta_gsr$gsr_pulse_timestamp_sync_unix_cal_ms


dta_psypy1$orig_unixtime_start_psypy<- dta_psypy1$unixtime_start

range(dta_psypy1$unixtime_start)
# hopefully we can use these to check that the merging should actuallymatch what we expect because the difference between this number and the timestamp should be close to zero where we actualy present the stimuli

# Rolling join 
# start with just one case
# db_time4leda$participant

dta_psypy1

colnames(dta_psypy1)

tmp_test<- dta_psypy1%>%
  subset(participant == 134)%>%
  select(participant,social_nonsocial,stim_iaps,trial_no_all,unixtime_start,orig_unixtime_start_psypy, unixtime_end)

tmp_test_gsr<- dta_gsr%>%
  subset(participant == 134)


setDT(tmp_test) 
setDT(tmp_test_gsr) 

options(scipen = 999)
tmp_test$orig_unixtime_start_psypy
# tmp_test$unixtime_start
# tmp_test$unixtime_end
# tmp_test_gsr$gsr_pulse_timestamp_sync_unix_cal_ms
tmp_test_psypy_gsr <-tmp_test[tmp_test_gsr, on = .( unixtime_start=  gsr_pulse_timestamp_sync_unix_cal_ms), 
                              roll = "nearest"]


tmp_test_psypy_gsr$orig_timestamp_dtaphyisio


# this means that unix time gets populated by the original physio time
dt_result$orig_timestamp_dtaphyisio-dt_result$unixtime_start

# the value close to zero here should be the start
dt_result$orig_timestamp_dtaphyisio-dt_result$orig_unixtime_start_psypy




tmp_test_psypy_gsr_test2<- dta_merged%>%
  subset(participant == 134)


tmp_test_psypy_gsr%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-orig_timestamp_gsr)%>%
  ggplot(aes(orig_timestamp_gsr, timediff_phys_start_psypy))+
  geom_point(size = .0001)

tmp_merged%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-orig_timestamp_gsr)%>%
  ggplot(aes(orig_timestamp_gsr, timediff_phys_start_psypy))+
  geom_point(size = .0001)




tmp_test_psypy_gsr%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-orig_timestamp_gsr)%>%
  ggplot(aes(orig_timestamp_gsr, gsr_pulse_gsr_skin_conductance_cal_u_s))+
  geom_line()+
  geom_vline(aes())
  

tmp_test2<- tmp_test_psypy_gsr%>%subset(trial_no_all== 10)
unique(tmp_test2$stim_iaps)
# so 
# 120*128
# 15360


tmp_merged%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-unixtime_start)%>%
  # group_by()
  # mutate(timenew = orig_timestamp_gsr[trigger!= 0])%>%
  ggplot(aes(orig_timestamp_gsr, gsr_pulse_gsr_skin_conductance_cal_u_s))+
  geom_line()+
  geom_vline(data = tmp_merged %>% filter(trigger != 0),
             aes(xintercept = orig_timestamp_gsr), color = "red")


# This code visualizes the alignment of physiological responses (GSR data) with experimental events (PsychoPy data) for the first two trials. It arranges the data by the start time of events, calculates the time differences between the PsychoPy and GSR timestamps, and visualizes these alongside GSR skin conductance levels. Vertical red lines highlight events or stimuli presentations with non-zero triggers, aiding in the examination of response timing relative to stimuli. This analysis is crucial for verifying the temporal accuracy of the experimental data alignment in the initial trials.


tmp_merged %>%subset(trial_no_all > 9 & trial_no_all < 15)%>%
  arrange(unixtime_start)%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-unixtime_start)%>%
  
      mutate(time = unixtime_start-unixtime_start[1])%>%
    # subset(time < 190000)%>%
  ggplot(aes(orig_timestamp_gsr, gsr_pulse_gsr_skin_conductance_cal_u_s,
             colour = trial_no_all))+
  geom_line()+
  geom_vline(data = tmp_merged %>% 
               filter(trigger != 0)%>%
               subset(trial_no_all > 9 & trial_no_all < 15),

             aes(xintercept = orig_timestamp_gsr), color = "red", size = .1)+
   geom_line()+
  geom_point(aes(y = timediff_phys_start_psypy/10000),size = .00000000001)

# this code shows that trial_no_all start appearing before the trial, this is ebacuse ofg the nar join we did earlier which essentially combines psychopy times based on near points, so the row wil be puished to the enares point and then replicated untyil it vhsanges this cases t
# so trial_number and posychopy information before the marker is largely innacurate and might eb safer to 1 - ignorte any additiopnal psychpopyn info oteh than just the time used to create start_true andn create 



options(scipen = 999)

tmp_merged<- tmp_merged%>%
  arrange(orig_timestamp_gsr)



dta_filtered<- dta_filtered%>%
  arrange(orig_timestamp_gsr)

tmp_merged$orig_timestamp_gsr[tmp_merged$start_true == TRUE]-

dta_filtered$gsr_pulse_timestamp_sync_unix_cal_ms[dta_filtered$start_true == TRUE]

# tmp_merged$tmp_merged$start_true == TRUE]-
tmp_merged

test_merged<- tmp_merged%>%
  subset(start_true == TRUE)%>%
  subset(!duplicated(stim_iaps))%>%
  
  arrange(orig_timestamp_gsr)

test_merged$orig_timestamp_gsr-
dta_filtered$orig_timestamp_gsr[dta_filtered$start_true == TRUE]
# 
# dta_filtered%>%subset(start_true == TRUE)
# dta_filtered$orig_timestamp_gsr-dta_filtered$gsr_pulse_timestamp_sync_unix_cal_ms
```




# trying a more efficient way of merging psychopy timsetamps to gsr using data.table

##### 
to worrk on
note as data gets bigger we will lielly need to do one file at a a time
##### 


```{r}
# Convert data.frames to data.tables for efficiency in large datasets


participants <- unique(dta_psypy1$participant)
# unique(tmp_test_psypy_gsr$stim_iaps)
participant_id = 134
p = 134
for (participant_id in participants) {
  # Filter participant data from each dataset
  tmp_psypy <- dta_psypy1[participant == participant_id]%>%
     select(participant,social_nonsocial,stim_iaps,trial_no_all,unixtime_start,orig_unixtime_start_psypy, unixtime_end)
  tmp_gsr <- dta_gsr[participant == participant_id]
  
  tmp_merged <- tmp_psypy[tmp_gsr, on = .(unixtime_start = gsr_pulse_timestamp_sync_unix_cal_ms), roll = "nearest"]%>%
    arrange(unixtime_start)
  
  # Compute time difference and establish start_true flag
  
  # Creating start_true flag based on timediff
# dta_merged[, timediff_phys_start_psypy := orig_unixtime_start_psypy - orig_timestamp_gsr]
# dta_merged[, start_true := fifelse(timediff_phys_start_psypy > 0 & timediff_phys_start_psypy <= 10, TRUE, NA)]
  # colnames(tmp_merged)
  tmp_merged[, timediff_phys_start_psypy := orig_unixtime_start_psypy - unixtime_start] #positive means stimuli hasnt started yet
  tmp_merged[, start_true := fifelse(timediff_phys_start_psypy > -10 & timediff_phys_start_psypy <= 0, TRUE,FALSE)]
  
  # Processing triggers based on key variables and conditions
  # tmp_merged$unixtime_end

  
# Number of samples=Sampling rate×Time in seconds
  # we can uswe this to say thjat we wilo look back 768 sampels *=(6 seconds to as ficxa)
  
  # just ignore the pre and opost responses for now
  # Process triggers
  
  # tmp_merged$unixtime_start - tmp_merged$orig_timestamp_gsr same
  tmp_merged <- tmp_merged %>%
    arrange(unixtime_start)%>%
    group_by(trial_no_all) %>%
    mutate(
           trigger = case_when(
             trial_no_all == 1 & start_true == TRUE ~ 1, 
             start_true == FALSE ~ 0, 
             TRUE ~ as.numeric(trial_no_all) # Default case if needed
           )) %>%
    ungroup()
  
  
  # Export the processed data for this participant
  fwrite(tmp_merged, paste0("processed_data_participant_", participant_id, ".csv"), row.names = FALSE)
}

```


note this below is not6 quite for ledalab  -see ahead
```{r}
library(data.table)

# Ensure dta_gsr and dta_psypy1 are data.tables
setDT(dta_gsr)
setDT(dta_psypy1)

participant_ids <- unique(c(dta_gsr$participant, dta_psypy1$participant))

# backup iorigial times
dta_gsr$orig_timestamp_gsr<- dta_gsr$gsr_pulse_timestamp_sync_unix_cal_ms

dta_psypy1$orig_unixtime_start_psypy<- dta_psypy1$unixtime_start



dta_psypy1


dta_gsr

# participant_id = 213
rm(participant_id)

setwd("/Users/pw22812/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/withtriggers")

for (participant_id in participant_ids) {
  
  tmp_gsr <- dta_gsr[participant == participant_id]
  tmp_psypy <- dta_psypy1[participant == participant_id][, .(participant, social_nonsocial, stim_iaps, trial_no_all, unixtime_start,orig_unixtime_start_psypy)]
  tmp_psypy[, next_unixtime_start := shift(unixtime_start, type = "lead", fill = Inf)]
  tmp_psypy[, unixtime_end := next_unixtime_start - 1]
  
  # Initialize columns in tmp_gsr to match the types from tmp_psypy
  tmp_gsr[, `:=` (social_nonsocial = as.character(NA), 
                  stim_iaps = as.character(NA), 
                  trial_no_all = as.integer(NA), 
                 psypy_unixtime_start = as.numeric(NA),
                  orig_unixtime_start_psypy = as.numeric(NA))]
  
  # Perform the non-equi join and update
  tmp_gsr[tmp_psypy, on = .(gsr_pulse_timestamp_sync_unix_cal_ms >= unixtime_start, 
                            gsr_pulse_timestamp_sync_unix_cal_ms <= unixtime_end), 
          `:=` (social_nonsocial = i.social_nonsocial, 
                stim_iaps = i.stim_iaps, 
                trial_no_all = i.trial_no_all, 
                psypy_unixtime_start = i.unixtime_start,
                orig_unixtime_start_psypy = i.orig_unixtime_start_psypy), 
          by = .EACHI]

  tmp_gsr<-tmp_gsr%>%
    ungroup()%>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms)%>%
    mutate(start_true = !duplicated(trial_no_all) & !is.na(trial_no_all))%>%
    mutate(trigger = case_when(
             trial_no_all == 1 & start_true == TRUE ~ 1, 
             start_true == FALSE ~ 0, 
             TRUE ~ as.numeric(trial_no_all)))%>%
    mutate(gsr_pulse_timerezero_sync_unix_cal_ms = gsr_pulse_timestamp_sync_unix_cal_ms-gsr_pulse_timestamp_sync_unix_cal_ms[1]) # rezero
  
  fwrite(tmp_gsr, paste0(participant_id,"_dta_gsr_trig.csv"), row.names = FALSE)
}
tmp_gsr$orig_timestamp_gsr
tmp_gsr$TR
```



just some checks
```{r}
tmp_gsr
unique(tmp_gsr$trial_no_all)

tmp_gsr$psypy_unixtime_start
colnames(tmp_gsr)
dta_filtered <- tmp_gsr %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms) %>%
  # Filter for trial numbers between 10 and 14 (inclusive)
  # filter(trial_no_all  <10) %>%
  # Group by 'trial_no_all' for operations within each trial
  group_by(trial_no_all) %>%
  # Arrange by timestamp within each trial group

  # Identify the first row in each trial group (start of the trial)
  # mutate(start_true = row_number() == 1) %>%
  # Create 'time' only for the first row in each group; NA otherwise
  # Identify the first non-NA occurrence in each group
  mutate(start_true = !duplicated(trial_no_all) & !is.na(trial_no_all)) %>%
  # Calculate the 'time' value only for the first valid occurrence
  
  mutate(time = ifelse(start_true, gsr_pulse_timestamp_sync_unix_cal_ms, NA)) %>%
  # Use `fill` to propagate the first timestamp down the group
  fill(time, .direction = "down")%>%
  ungroup()%>%
  arrange(gsr_pulse_timestamp_sync_unix_cal_ms)

# Now, plot with 'ggplot2'
dta_filtered$orig_unixtime_start_psypy-dta_filtered$orig_timestamp_gsr


dta_filtered$participant

dta_filtered%>%
  subset(trial_no_all>15 & trial_no_all<20)%>%
  mutate(time_diff_psypy_gsr = orig_unixtime_start_psypy-orig_timestamp_gsr)%>%
  ggplot( aes(x = gsr_pulse_timestamp_sync_unix_cal_ms, 
                         y = gsr_pulse_gsr_skin_conductance_cal_u_s)) +
  geom_line() +
  # Add red vertical lines at 'time' for the start of each trial
  geom_vline(aes(xintercept = time), color = "red") +
  geom_vline(aes(xintercept = time+6500), color = "blue") +
  geom_vline(aes(xintercept = time+1000), linetype = "dashed")+
geom_vline(aes(xintercept = time-1000), linetype = "dashed")
  # geom_vline(xintercept = 1709728450000)+
      # xlim(1709728450000,1709728660000-100000)
  # labs(colour = "Trial Number")+
  # geom_point(aes(y = time_diff_psypy_gsr/100000), size = .000000000000001)

  
  tmp_merged%>%
      subset(trial_no_all>10 & trial_no_all<18)%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-unixtime_start)%>%
  # group_by()
  # mutate(timenew = orig_timestamp_gsr[trigger!= 0])%>%
  ggplot(aes(orig_timestamp_gsr, gsr_pulse_gsr_skin_conductance_cal_u_s))+
  geom_line()+
  geom_vline(data = tmp_merged %>% filter(trigger != 0 & trial_no_all>10 & trial_no_all<18),
             aes(xintercept = orig_timestamp_gsr), color = "red")+
     geom_vline(data = tmp_merged %>% filter(trigger != 0 & trial_no_all>10 & trial_no_all<18),
             aes(xintercept = orig_timestamp_gsr+6500), color = "blue")+
    geom_vline(data = tmp_merged %>% filter(trigger != 0 & trial_no_all>10 & trial_no_all<18),
             aes(xintercept = orig_timestamp_gsr+1000), linetype = "dashed")+
      geom_vline(data = tmp_merged %>% filter(trigger != 0 & trial_no_all>10 & trial_no_all<18),
             aes(xintercept = orig_timestamp_gsr-1000), linetype = "dashed")+
    
    geom_vline(xintercept = 1709728450000)+
    xlim(1709728450000,1709728660000-100000)



```

this actually works and agrees with the previopusnone
just with this for now, but we need to investigate wether this is happening slighlly after we intend it




write to ledalab

# Data Preparation for Ledalab Analysis
# Note: 
# - Ledalab has very specific format requirements that cna create issues= needs .txt without column names.
# - The presence of 'NA' values, weird characters, or numbers as characters (e.g "1") even empty spaces can cause loading, opening, or importing issues within Ledalab. It may lead to repeated prompts for specifying variables, which indicates these common problems.
# - It is crucial to meticulously prepare and clean the data to meet these specific requirements to avoid any operational disruptions in Ledalab.

# Comparative Validation Note:
# - A manual examination was conducted between a dataset processed mkanually (participant 110) and another processed through this automated script to gtenearte trigger. This was done to ensure that the start times, particularly for GSR (Galvanic Skin Response) data, accurately match when analyzed in Ledalab.
# - This comparison confirmed that both the timing and the pattern of analysis remain consistent between manually processed and automated datasets. 


for ledalab, we'll just write time stamp (original and re zeroed, skin conductance data) and triggers, anyt medatdasta wil be matched later - based on trigger (consider writing ann aggreate macthign file from trigger to psypy?)

this one below export the fulld ataset, but its best to export rest and trials separatelly
```{r eval=FALSE, include=FALSE}

colnames(tmp_gsr)


unique(X126_dta_gsr_trig$trigger)

# Define the directory where participant CSV files are stored
setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/withtriggers")

input_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/withtriggers"

# Define the output directory for the formatted files
output_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/withtriggers/format4leda"

library(dplyr)

# List all CSV files in the directory
participant_files <- list.files(input_directory, pattern = "\\.csv$", full.names = TRUE)

# Function to process each file
process_participant_file <- function(file_path) {
  # Read the CSV file
  data <- read.csv(file_path)%>%
    select(gsr_pulse_timerezero_sync_unix_cal_ms,
           gsr_pulse_timestamp_sync_unix_cal_ms,
           gsr_pulse_gsr_skin_conductance_cal_u_s,
           trigger)
  
  
  # Arrange by unixtime_start if needed
  selected_data <- data %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms)
  
  # Construct output file name based on input file name and output directory
 
 # Remove the file extension and append "_ledaform.txt" to construct the output file name
  file_name_no_ext <- tools::file_path_sans_ext(basename(file_path))
  output_file_name <- paste0(output_directory, "/", file_name_no_ext, "_ledaform.txt")
  
  
  # Write the selected and potentially arranged data to a new file, format for Ledalab
  write.table(selected_data, 
              output_file_name, sep = "\t",
              row.names = FALSE, 
              col.names = FALSE)
  # Print a message indicating progress
  cat("Processed and written:", output_file_name, "\n")
}

# Apply the function to each file
lapply(participant_files, process_participant_file)
```


export trials and rest period separatelly
```{r}


input_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/withtriggers"

# Define the output directory for the formatted files
output_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated/withtriggers/format4leda"
# Assuming input_directory and output_directory are defined
participant_files <- list.files(input_directory, pattern = "\\.csv$", full.names = TRUE)

process_participant_file <- function(file_path) {
  # Read the CSV file and select relevant columns
  data <- read.csv(file_path) %>%
    select(gsr_pulse_timerezero_sync_unix_cal_ms,
           gsr_pulse_timestamp_sync_unix_cal_ms,
           gsr_pulse_gsr_skin_conductance_cal_u_s,
           trigger) %>%
    arrange(gsr_pulse_timestamp_sync_unix_cal_ms)

  # Find indices for trigger conditions
  first_trigger_2_index <- which(data$trigger == 2)[1]
  first_trigger_1_index <- which(data$trigger == 1)[1]

  # Function to rezero time
  rezero_time <- function(df) {
    df %>%
      mutate(gsr_pulse_timerezero_sync_unix_cal_ms = gsr_pulse_timerezero_sync_unix_cal_ms -
               gsr_pulse_timerezero_sync_unix_cal_ms[1])
  }

  # Data before first occurrence of trigger == 2
  if (!is.na(first_trigger_2_index) && first_trigger_2_index > 0) {
    data_before_trigger_2 <- data %>%
      slice(1:(first_trigger_2_index - 1)) %>%
      rezero_time()

    # Construct output file name for data before trigger == 2
    file_name_no_ext <- tools::file_path_sans_ext(basename(file_path))
    output_file_name_before_trigger_2 <- paste0(output_directory, "/", file_name_no_ext, "_rest_ledaform.txt")

    # Write to file
    write.table(data_before_trigger_2, output_file_name_before_trigger_2, sep = "\t", row.names = FALSE, col.names = FALSE)
    cat("Processed and written (before trigger 2):", output_file_name_before_trigger_2, "\n")
  }

  # Data after first occurrence of trigger == 1, excluding the trigger row itself
  if (!is.na(first_trigger_1_index) && first_trigger_1_index > 0) {
    data_after_trigger_1 <- data %>%
      slice((first_trigger_1_index + 1):n()) %>%
      rezero_time()

    # Construct output file name for data after trigger == 1
    output_file_name_after_trigger_1 <- paste0(output_directory, "/", file_name_no_ext, "_pract_nd_trials_ledaform.txt")

    # Write to file
    write.table(data_after_trigger_1, output_file_name_after_trigger_1, sep = "\t", row.names = FALSE, col.names = FALSE)
    cat("Processed and written (after trigger 1):", output_file_name_after_trigger_1, "\n")
  }
}


# Process each file
lapply(participant_files, process_participant_file)

```


# ==============================================================================
# README: Preparation and Analysis of Ledalab Formatted Files
# ==============================================================================
ledaform files
GSR files were processed to select relevant columns and arrange data by timestamp, ensuring compatibility with Ledalab software. Output files are saved without column names, using tab-separated values (TSV), in line with Ledalab's input requirements. note ledalab has an issue wityh NAs, strringds, empty columsn or even numnumebrsebrs wrriten like "11".
any issues with loading likely has to do with that

### Output File Format:
Each file is named after the original participant data file, appended with "_ledaform.txt" to indicate formatting for Ledalab. The files include the following columns:
  1. **gsr_pulse_timerezero_sync_unix_cal_ms**: Timestamp (Unix time) after rezeroing, indicating synchronized GSR pulse time.
  2. **gsr_pulse_timestamp_sync_unix_cal_ms**: Original Unix timestamp of the GSR pulse.
  3. **gsr_pulse_gsr_skin_conductance_cal_u_s**: Calibrated skin conductance level in microsiemens (μS).
  4. **trigger**: Event marker or trigger signal associated with the measurement.

### Sampling Rate and Downsampling:
- **Original Sampling Rate**: 128Hz.
- **Downsampling Target**: 16Hz (or the lowest rate that maintains a frequency above 10Hz), corresponding to a downsampling factor of 8 (128Hz / 8 = 16Hz).
- **Method**: Use "factor mean" in Ledalab for downsampling, averaging data points within each factor to preserve signal characteristics.

## Importing Data into Ledalab:
Select 'Data Type 3' (manual definition) upon import to manually specify data structure for accurate interpretation and analysis within Ledalab.

# Analysis Guidelines for Ledalab:

## Preliminary Manual Analysis (GUI):
Before batch processing, it's essential to manually analyze a one or two files to establish a benchmark for comparison and ensure the analysis settings are correctly configured. This will also serve as a referecne for automated analysis

### Steps for Manual Analysis:
1. **Open Ledalab's GUI**: Load a files from 'rest' and 'pract_nd_trial' categories to familiarize with the data and settings.
2. **Configure Analysis Settings**:
   - Enable **Adaptive Smoothing**: Adjusts the smoothing parameter dynamically for each data segment to improve the signal-to-noise ratio.

3. **Run CDA Analysis**: Perform Continuous Decomposition Analysis to decompose the GSR signal into its tonic and phasic components.
   - **Optimize Twice**: Enhances the accuracy of the Continuous Decomposition Analysis (CDA) by refining the decomposition parameters iteratively.
4. **Export event related activation**:
   - For trials: Analyze event-related data within a window of **1 to 7 seconds** (or **0.5 to 7 seconds** if permissible).
   - For rest: Use a window of **1 to 120 seconds** to capture the full range of resting data.
5. **Export and Save Results**: Ensure to save `.mat` files for the analysis results and key plots for later comparison
also save a mat file of the whole analysis

## Automated Batch Processing:
After manual analysis, automate the processing for all files using Ledalab's batch mode. This ensures consistent application of the predetermined settings across all files.

### Batch Processing Steps:
- Visit [Ledalab Batch Mode Documentation](http://www.ledalab.de/documentation.htm#batchmode) for detailed instructions on setting up batch processing.

#### Sample Batch Processing Commands: check the slashes as t5hey differ on windows and mac

- **For Practice and Trials**:
Ledalab('D:\OneDrive - Nexus365\InteroStudy2020\analysis\DataAnalysisJanuary2020\DataAnalysisJan2020\GP_gsr_forLeda', 'open','text3','downsample', 15, 'smooth',{'adapt'}, 'analyze','CDA', 'optimize',3, 'export_era', [1 7 .01 2],  'overview',1)

Code for automatic rest analysis
Ledalab('D:\OneDrive - Nexus365\InteroStudy2020\analysis\DataAnalysisJanuary2020\DataAnalysisJan2020\GP_gsr_forLeda', 'open','text3','downsample', 15, 'smooth',{'adapt'}, 'analyze','CDA', 'optimize',3, 'export_era', [1 120 .01 2],  'overview',1)


insopect some fo teh saved ouputs and plots and liusyt of problem files any files you can't run in batch mode run manualy


combine the ledalab output with behavioural data and check if we have what we would expect

3. Plot Physio data in R and Ledalab

```{r}

dta_leda_era_110 <- read_delim("dt_time_trig_gsr_pre_post_trig_era.txt")

data_psypy1



dta_leda_era_110<- dta_leda_era_110%>%
  janitor::clean_names()

dta_leda_era_110$trigger<- as.numeric(dta_leda_era_110$event_nid)

dta_leda_era_110%>%
  mutate(pre_post_stim = if_else(dta_leda_era_110$trigger<100, "stim",
                                 if_else(dta_leda_era_110$trigger>100 & dta_leda_era_110$trigger< 900, 
                                         "post", "pre")
                                 ))%>%
  mutate(cda_amp_sum_z = scale(cda_amp_sum))%>%
   mutate(cda_amp_sum_z_post = lead(cda_amp_sum_z))%>%
  subset(pre_post_stim = "stim")%>%
 ggplot(aes(pre_post_stim, cda_amp_sum_z))+
  stat_summary(geom = "pointrange")


dta_leda_era_110%>%
  mutate(pre_post_stim = if_else(dta_leda_era_110$trigger<100, "stim",
                                 if_else(dta_leda_era_110$trigger>100 & dta_leda_era_110$trigger< 900, 
                                         "post", "pre")
                                 ))%>%
  mutate(cda_amp_sum_z = scale(cda_amp_sum))%>%
   mutate(cda_amp_sum_z_post = lead(cda_amp_sum_z))%>%
  subset(pre_post_stim = "stim")%>%
 ggplot(aes(cda_amp_sum_z, cda_amp_sum_z_post))+
  geom_point()+
  geom_smooth(method = 'lm', se = F)


dta_leda_era_110%>%
  mutate(pre_post_stim = if_else(dta_leda_era_110$trigger<100, "stim",
                                 if_else(dta_leda_era_110$trigger>100 & dta_leda_era_110$trigger< 900, 
                                         "post", "pre")
                                 ))%>%
  mutate(cda_amp_sum_z = scale(cda_amp_sum))%>%
   mutate(cda_amp_sum_z_post = lag(cda_amp_sum_z))%>%
  subset(pre_post_stim = "stim")%>%
 ggplot(aes(cda_amp_sum_z, cda_amp_sum_z_post))+
  geom_point()+
  geom_smooth(method = 'lm', se = F)

```


