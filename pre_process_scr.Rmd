---
title: "pre_process SCR shimmer data - social emophys"
author: "Helio"
date: "2024-03-07"
output: html_document
---



2. Export/create triggers from behavioural data (merge physio with psychopy)

COMBINE TRIGGERS WITH PHYSIO


Set-up: where I load libraries and create objects for later use (such as color schemes etc).
```{r setup, include=FALSE}
# Load necessary libraries
library(tidyverse)
library(data.table)
library(janitor)
library(purrr)

# Define paths
data_path <- "./Data backup/DataChecks/R code and data/feb 6"
ctrl_data_path <- file.path(data_path, "ctrl")


```


load files 
```{r}
# data_psypy1

base_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio" # Change this to your actual base directory path


# List immediate subdirectories first (non-recursive)
# subdirs <- list.dirs(path = base_directory, recursive = FALSE, full.names = TRUE)

# Initialize a vector to hold directories that match the "Export" pattern
export_dirs <- c()

# Now, for each immediate subdir, look one level down for "Export" folders
for (dir in subdirs) {
  cat("Checking in:", dir, "\n") # Simple progress feedback
  # Assuming "Export" directories are directly under these subdirs
  potential_exports <- list.dirs(path = dir, recursive = FALSE, full.names = TRUE)
  matched_exports <- grep("Consensys", potential_exports, value = TRUE)
  export_dirs <- c(export_dirs, matched_exports)
}

# export_dirs now contains paths to directories matching the "Export" pattern
# Find all sub directories named "raw" within the base directory

export_dirs

# export_dir <- list.files(path = base_directory, 
#                               pattern = "Export", 
#                               full.names = TRUE, 
#                               recursive = TRUE, 
#                               include.dirs = TRUE)


# Custom function to read a file and adjust its column names
fn_read_and_adjust <- function(file_path) {
  # Read the first two rows to adjust column names
  header_info <- fread(file_path, nrows = 1, skip = 1, header = FALSE)
  col_names <- colnames(fread(file_path, nrows = 1, skip = 2, header = FALSE))
  
  # Modify column names based on the header info
  new_col_names <- paste0(col_names, "_", unlist(header_info[1,]))
  
  # Now read the full data (skipping the first two rows) and assign new column names
  data <- fread(file_path, skip = 2, header = FALSE)
  setnames(data, old = colnames(data), new = new_col_names)
  
  # Return the modified data.table
  return(data)
}



```

recursive folder search
```{r}
base_directory <- "~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio"

# Ensure this directory exists and R can access it. If there are access issues, consider absolute paths or check permissions.
# List immediate subdirectories first (non-recursive)
subdirs <- list.dirs(path = base_directory, recursive = FALSE, full.names = TRUE)


# Initialize an empty vector to store the paths of all desired CSV files
csv_files_paths <- c()

# For each "Export" directory, look one level deeper
for (export_dir in export_dirs) {
    # List subdirectories within the current "Export" directory
    subdirs_in_export <- list.dirs(path = export_dir, recursive = FALSE, full.names = TRUE)
    
    # Filter out directories that contain "uncalibrated" in their name
    filtered_subdirs <- grep("uncalibrated", subdirs_in_export, value = TRUE, invert = TRUE, ignore.case = TRUE)
    
    # For each filtered subdirectory, list CSV files
    for (subdir in filtered_subdirs) {
        # List all CSV files in the subdirectory
        csv_files_in_subdir <- list.files(path = subdir, pattern = "\\.csv$", full.names = TRUE)
        
        # Add the found files to the collection
        csv_files_paths <- c(csv_files_paths, csv_files_in_subdir)
    }
}

# Proceed with applying the custom function to the list of CSV files

# Apply the custom function to each CSV file and combine the results

# grep("_PC",csv_files_paths)
# 
# tmp_test <-csv_files_paths[grep("_PC",csv_files_paths)]
# 
# tmp_test[grep("GSR",tmp_test)]

# combined_data <- rbindlist(lapply(csv_files_paths, fn_read_and_adjust), use.names = TRUE, fill = TRUE, idcol = "filename") %>%
#   janitor::clean_names()

# 'combined_data' now contains your adjusted and combined dataset



# why is 217 000



```


simpler approach 
- search on mac and copy easily to a folder called calibrated
- note check logbook for notes on data issues


GSR data should be sampled at 128 hz 

# 1000 ms = 128
# 6000 ms = 128*6 = 768
```{r}

setwd("~/Library/CloudStorage/OneDrive-UniversityofBristol/Research Projects/EmotionPhysio2024/2023-24 Project/Data backup/Consensys-physio/Calibrated")
# List of .csv files, assuming you've already created 'tmp_files' with list.files()
# tmp_files <- list.files(pattern = "\\._PCcsv$", recursive = FALSE, full.names = TRUE)
tmp_files <- list.files(pattern = "GSR.*PC\\.csv$", recursive = FALSE, full.names = TRUE)

tmp_files_SD <- list.files(pattern = "GSR.*SD\\.csv$", recursive = FALSE, full.names = TRUE)

tmp_files
tmp_files_SD

rm(fn_extract_identifiers)

# Extract identifiers

# Regular expression to capture the pattern before "_" and after "./"
pattern <- "\\./([0-9]+)_.*"

# Extract the pattern
identifiers_tmp_files_PC <- sub(pattern, "\\1",  tmp_files)
identifiers_tmp_files_SD <- sub(pattern, "\\1", tmp_files_SD)

# Compare and find common identifiers
common_identifiers <- intersect(identifiers_tmp_files_PC, identifiers_tmp_files_SD)

# Display the common identifiers
common_identifiers

# Identifiers unique to PC files
unique_identifiers_PC <- setdiff(identifiers_tmp_files_PC, identifiers_tmp_files_SD)

# Identifiers unique to SD files
unique_identifiers_SD <- setdiff(identifiers_tmp_files_SD, identifiers_tmp_files_PC)

# Display the unique identifiers
unique_identifiers_PC
unique_identifiers_SD

# [1] "101" "102" "103" "106"
# > unique_identifiers_SD
# [1] "105"
# copy and rename 105 sd to ad _PC, so we can read all pcs again

# then use this info to load PC files and just add sd wheere needed

```

load gsr data
```{r}
###################################################
# Handling file imports from PC and SD sources:
# - This process primarily relies on PC files, but switches to SD files in the absence of a corresponding PC file.
# - A notable discrepancy has been observed in the export formats: PC exports contain "sync" within the timestamps, unlike SD exports. This difference may affect how filenames are generated and could imply variations in how data synchronization is handled or represented between the two sources.
# - It is essential to consider this variation during data processing to ensure consistent naming conventions and to investigate the synchronization status of SD data.
# - The custom function is applied to each file to adjust for these discrepancies, and the results are subsequently combined.

###################################################

tmp_files <- list.files(pattern = "GSR.*PC\\.csv$", recursive = FALSE, full.names = TRUE)
tmp_files

# do we actually want to store all of thes efiles here, or just load each adjut then save it back due to memory issues
# try again

# should have removed this before running the code below
rm(dta_gsr)

# Custom function to read a file and adjust its column names
# Function to read a dataset and adjust column names based on the first two rows
# Overview:
# 1. Reads the first two rows of the file to use as basis for creating unique column names.
# 2. Combines these two rows to form a single set of column names for the dataset.
# 3. Reads the entire dataset, skipping the first two rows which were used for column names.
# 4. Assigns the combined column names to the dataset.
# 5. Adds a column indicating the filename from which the data was read.
# 6. This approach ensures proper data type handling by skipping header rows during the full data read.


library(data.table)
library(dplyr)
library(janitor)

# Function to read a dataset and adjust column names based on the first two rows
# Overview:
# 1. Reads the first two rows of the file to use as basis for creating unique column names.
# 2. Combines these two rows to form a single set of column names for the dataset.
# 3. Reads the entire dataset, skipping the first two rows which were used for column names.
# 4. Assigns the combined column names to the dataset.
# 5. Adds a column indicating the filename from which the data was read.
# 6. This approach ensures proper data type handling by skipping header rows during the full data read.
fn_read_and_adjust <- function(file_path) {
  # Read the first two rows for column names adjustment
  header_info <- fread(file_path, nrows = 2, header = FALSE)
  
  # Combine the first two rows to form the column names, enhancing uniqueness
  col_names <- paste0(unlist(header_info[1,]), "_", unlist(header_info[2,]))
  
  # Read the full dataset, explicitly skipping the first two header rows
  data <- fread(file_path, skip = 3, header = FALSE) #using 3 just to be sure
  setnames(data, old = colnames(data), new = col_names) # Assign new column names
  
  # Add a column with the filename to keep track of the data source
  data[, filename := basename(file_path)]
  
  return(data)
}


# Process a list of file paths, adjust their column names, and combine into a single data.table
# Clean column names using `janitor::clean_names` for consistency
# Assuming tmp_files is a list of file paths
tmp_files
dta_gsr <- rbindlist(lapply(tmp_files, fn_read_and_adjust), use.names = TRUE, fill = TRUE) %>%
  janitor::clean_names()%>%
  select(filename, matches("gsr"))



colnames(dta_gsr)
unique(dta_gsr$filename)

head(dta_gsr)
dta_gsr$filename
dta_gsr<- dta_gsr%>%
  mutate(participant = sub("_.*", "", filename))



# Replace '000' with '217' in the participant column
# 000 is 217 coded by mistake by the students
dta_gsr$participant <- ifelse(dta_gsr$participant == '000', '217', dta_gsr$participant)


unique(dta_gsr$participant)
# 
# [1] "217" "101" "102" "103" "104" "105" "106" "107" "108" "109" "110" "111" "112" "113"
# [15] "114" "115" "116" "117" "118" "120" "121" "122" "123" "124" "125" "126" "127" "128"
# [29] "129" "130" "131" "132" "133" "134" "135" "136" "201" "202" "203" "204" "205" "206"
# [43] "207" "208" "209" "210" "211" "212" "213" "214" "215" "216" "218" "219" "220" "221"
# [57] "222" "224" "225" "226" "227" "228" "229" "230" "231" "232" "233" "234"

```


# assuming 1.5 hours times 93 peoples recordinds made at 128 hz, how many rows should we expect this data to have

# Given values
# hours_per_recording = 1.5
# seconds_per_hour = 3600
# sampling_rate_hz = 128
# number_of_participants = 93
# 
# # Calculate total seconds per recording
# seconds_per_recording = hours_per_recording * seconds_per_hour
# 
# # Calculate number of samples per recording
# samples_per_recording = seconds_per_recording * sampling_rate_hz
# 
# # Calculate total number of rows for all recordings
# total_rows = samples_per_recording * number_of_participants
# total_rows
# 
# 64281600.0
# less than expected
# nrow(dta_gsr)/64281600.0


# issue here, when using a different sensor, e.g. fd29 the columns are named ioncomnsistently 
# e.g. v3_shimmer_fd29_gsr_range_cal" ratyher than v3_gsr_pulse_gsr_range_cal"  
# so these ietehr need to be renamed manually or programatically

combine with timestamps from psychopy

```{r}


# keep some original time

dta_gsr$orig_timestamp_gsr<- dta_gsr$gsr_pulse_timestamp_sync_unix_cal_ms


dta_psypy1$orig_unixtime_start_psypy<- dta_psypy1$unixtime_start

range(dta_psypy1$unixtime_start)
# hopefully we can use these to check that the merging should actuallymatch what we expect because the difference between this number and the timestamp should be close to zero where we actualy present the stimuli

# Rolling join 
# start with just one case
# db_time4leda$participant

dta_psypy1

colnames(dta_psypy1)

tmp_test<- dta_psypy1%>%
  subset(participant == 134)%>%
  select(participant,social_nonsocial,stim_iaps,trial_no_all,unixtime_start,orig_unixtime_start_psypy, unixtime_end)

tmp_test_gsr<- dta_gsr%>%
  subset(participant == 134)


setDT(tmp_test) 
setDT(tmp_test_gsr) 

options(scipen = 999)
tmp_test$orig_unixtime_start_psypy
# tmp_test$unixtime_start
# tmp_test$unixtime_end
# tmp_test_gsr$gsr_pulse_timestamp_sync_unix_cal_ms
tmp_test_psypy_gsr <-tmp_test[tmp_test_gsr, on = .( unixtime_start=  gsr_pulse_timestamp_sync_unix_cal_ms), 
                              roll = "nearest"]


tmp_test_psypy_gsr$orig_timestamp_dtaphyisio


# this means that unix time gets populated by the original physio time
dt_result$orig_timestamp_dtaphyisio-dt_result$unixtime_start

# the value close to zero here should be the start
dt_result$orig_timestamp_dtaphyisio-dt_result$orig_unixtime_start_psypy

tmp_test_psypy_gsr%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-orig_timestamp_gsr)%>%
  ggplot(aes(orig_timestamp_gsr, timediff_phys_start_psypy))+
  geom_point(size = .0001)


tmp_test_psypy_gsr%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-orig_timestamp_gsr)%>%
  ggplot(aes(orig_timestamp_gsr, gsr_pulse_gsr_skin_conductance_cal_u_s))+
  geom_line()


tmp_test_psypy_gsr_test2<- dta_merged%>%
  subset(participant == 134)
```


library(dplyr)
library(data.table)
library(ggplot2)

# Assuming dta_psypy1 and dta_gsr are already loaded and prepared
```{r}
# Convert data.frames to data.tables for efficiency in large datasets
setDT(dta_psypy1)
setDT(dta_gsr)

# Using a rolling join to merge datasets based on time proximity
dta_merged <- dta_psypy1[dta_gsr, on = .(participant, unixtime_start = gsr_pulse_timestamp_sync_unix_cal_ms), roll = "nearest"]





# Perform a rolling join within each participant
# Assume 'participant' is your grouping variable and 'unixtime_start' & 'gsr_pulse_timestamp_sync_unix_cal_ms' are the time variables
rm(dta_merged2)


# Creating start_true flag based on timediff
dta_merged[, timediff_phys_start_psypy := orig_unixtime_start_psypy - orig_timestamp_gsr]
dta_merged[, start_true := fifelse(timediff_phys_start_psypy > -10 & timediff_phys_start_psypy <= 0, TRUE, NA)]



# with warning


# Create `start_true` flag based on timediff_phys_start_psypy
dta_merged[, timediff_phys_start_psypy := unixtime_start - gsr_pulse_timestamp_sync_unix_cal_ms]

# Initially set all as NA to handle cases where the condition doesn't apply
dta_merged[, start_true := NA]

# Update start_true based on the condition, within each participant
dta_merged[, start_true := fifelse(timediff_phys_start_psypy > -10 & timediff_phys_start_psypy <= 0, TRUE, start_true), by = participant]

# Check if the condition is not met for any participant and issue a warning
if (any(is.na(dta_merged$start_true))) {
  warning("The condition for 'start_true' is not met for some participants.")
}


# Plotting (Example)
ggplot(dta_merged, aes(x = orig_timestamp_gsr, y = timediff_phys_start_psypy)) +
  geom_point(aes(color = start_true), size = 1) +
  geom_vline(data = dta_merged[start_true == TRUE], aes(xintercept = orig_timestamp_gsr), linetype = "dashed", color = "red") +
  labs(title = "Time Difference by Original GSR Timestamp", x = "Original GSR Timestamp", y = "Time Difference from PsyPy Start")

# Remember to adjust column names and conditions according to your specific dataset structure
```


# create a range of likely start values use as start

```{r}
# this is less than 10 milliseconds and we use negative so that the stimuli starts slightly before, but we can also do after
tmp_test_psypy_gsr$start_true<- if_else(tmp_test_psypy_gsr$timediff_phys_start_psypy> -10 & 
                                 tmp_test_psypy_gsr$timediff_phys_start_psypy <=0, TRUE, NA)
# add something to checl that we have values in this range, if we don't sent a warning



dt_result%>%
  mutate(timediff_phys_start_psypy = orig_unixtime_start_psypy-orig_timestamp_dtaphyisio)%>%
  subset(trial_no_pract_and_test == 3)%>%
  ggplot(aes(orig_timestamp_dtaphyisio, timediff_phys_start_psypy))+
  geom_point(size = .01)
  geom_vline(aes(xintercept = orig_timestamp_dtaphyisio[start_true == TRUE]))
 geom_vline(aes(xintercept = (orig_timestamp_dtaphyisio[start_true == TRUE])+6000))

```

```{r}

library(dplyr)

# create new variable whioch is TRUE when start of the stim is section is TRUE
dt_result2<- dt_result %>%
  # Apply the condition for start_true directly in the mutation
  group_by(stim_iaps) %>%
  # Create a helper column for the absolute value of timediff_phys_start_psypy
  mutate(abs_timediff = abs(timediff_phys_start_psypy)) %>%
  # Create the new variable where start_true is TRUE only for the smallest absolute timediff
  mutate(new_variable = start_true & (abs_timediff == min(abs_timediff[start_true], na.rm = TRUE))) %>%
  # Replace TRUE with NA where start_true is not TRUE
  mutate(new_variable = ifelse(start_true, new_variable, NA_logical_))%>%
  # select(-abs_timediff) %>% # Optional: Remove the helper column
  ungroup()
    

dt_result2

table(dt_result2$new_variable)
# fix resp start

dt_result2$unixtime_end<- if_else(dt_result2$condition == "rest_started",
                                 round(dt_result2$unixtime_end*1000),
                                 dt_result2$unixtime_end)
dt_result2%>%
  subset(new_variable == TRUE & trial_no_pract_and_test>0)%>%
  mutate(diff_end = orig_timestamp_dtaphyisio+6500-unixtime_end)%>%
  ggplot(aes(trial_no_pract_and_test, diff_end/1000))+
  geom_point()


# the fact that the unix time end - timestamp end is less than 10 milliseconds suggests that the differemnce. ois about 0

dt_result2$new_variable
dt_result%>%
  mutate()
  


```

# lets take the lowest negative value as the actual start

```{r}

dt_result2 %>%
  # Ensure start_true is a logical column (TRUE/NA)
  mutate(start_true = !is.na(start_true) & start_true) %>%
  arrange(unixtime_start)%>%
  subset(new_variable == TRUE)

# create a trigger, add zero when the trigger is off, this is important for ledalab

table(is.na(dt_result2$trial_no_pract_and_test))
dt_result2<- dt_result2%>%
  mutate(pre_trigger = trial_no_pract_and_test+1)%>%
  mutate(trigger = if_else(new_variable == TRUE, pre_trigger, 0))

table(dt_result2$trigger)
# lets create a fixation trigger (anticipation by 6 seconds)

dt_result2$unixtime_start
table(dt_result2$new_variable)

table(dt_result2$start_true)
dt_result2$stim_start_true<- dt_result2$new_variable

table(dt_result2$stim_start_true)
# first we return the time when stim_start_true is TRUE then substract 6 seconds
# a potentially simpler weay to do this is by digureing our how mnay rows should 6 s (6000 ms) be at 128 hz

# 1000 ms = 128
# 6000 ms = 128*6 = 768



library(dplyr)

dt_result3<- dt_result2%>%
  mutate(stim_event_index = if_else(stim_start_true == TRUE, row_number(), NA_integer_)) %>%
  fill(stim_event_index, .direction = "up") %>%
  mutate(pre_stim_start_true = if_else(row_number() == (stim_event_index - 768), TRUE, FALSE),
         trigger = if_else(pre_stim_start_true == TRUE, 999, trigger))



dt_result3<- dt_result2%>%
    mutate(unixtime_start_6000 = unixtime_start-6000)%>%
  mutate(pre_stim_start_true_time = if_else(stim_start_true == TRUE,
                                           unixtime_start, NA))%>%
arrange(unixtime_start)%>%
    fill(pre_stim_start_true_time, .direction = "up")%>%
 

  mutate(pre_stim_start_true  = if_else(pre_stim_start_true_time-6000 ==unixtime_start_6000, TRUE,FALSE))%>%

  mutate(trigger = if_else(pre_stim_start_true == TRUE, 999,trigger))


    # subset(stim_start_true == TRUE)
 # select(c(unixtime_start, pre_stim_start_true_time,unixtime_start_6000, stim_start_true, pre_stim_start_true,))

# dt_result2$trigger
table(dt_result3$trigger)
# dt_result2$trigger<- if_else(!is.na(dt_result2$trigger), dt_result2$trigger, 0)
# table(dt_result2$trigger)



# 999 is fixation
dt_result3$unixtime_end
dt_result3$unixtime_start

(dt_result3$unixtime_end/1000)/1000
table(dt_result3$unixtime_start == dt_result3$unixtime_end)
1706867600786-1706867866597
# now response time

range(dt_result3$unixtime_end)


dt_result3%>%
  subset(condition  != "rest_started" )%>%
  select(unixtime_end)
# dt_result3$unixtime_end<- if_else(dt_result3$condition != "rest_started", 
#                                             round(dt_result3$unixtime_end*1000),
#                                             dt_result3$unixtime_end)

dt_result3$stim_iaps

dt_result3<- dt_result3%>%
    group_by(stim_iaps)%>%
  mutate(resp_start_true = unixtime_start>unixtime_end)%>%
  mutate(resp_start_true_sup = !duplicated(resp_start_true))

table(dt_result3$resp_start_true)

table(dt_result3$resp_start_true_sup)

range(dt_result3$unixtime_end)
table(dt_result3$trigger)



dt_result3%>%
  subset(pre_stim_start_true == TRUE)%>%
  select(unixtime_start, pre_stim_start_true, stim_start_true,trigger,target_time_for_stim_start)



library(dplyr)

dt_result3 <- dt_result3 %>%
  group_by(stim_iaps) %>%
  mutate(resp_start_true = unixtime_start > unixtime_end) %>%
  mutate(first_true_flag = cumsum(resp_start_true) == 1) %>%
  mutate(resp_start_true_sup = if_else(resp_start_true & first_true_flag, TRUE, FALSE)) %>%
  ungroup()


dt_result4<- dt_result3
unique(dt_result3$trigger)

table(dt_result4$resp_start_true_sup )
table(dt_result4$trigger )

unique(dt_result4$trigger)
dt_result4$trigger1<- if_else(is.na(dt_result4$trigger) & 
                            
                            dt_result4$resp_start_true_sup == TRUE,
                            111,
                            dt_result4$trigger)


dt_result4$trigger1<- if_else(is.na(dt_result4$trigger1),0,
                            dt_result4$trigger1)


table(dt_result4$trigger1)


unique(dt_result3$trigger)



dt_result4$trigger<-dt_result4$trigger1


  
unique(dt_result3$trigger)


dt_result3%>%
  subset(condition == "rest_started")
  subset(resp_start_true == TRUE)
```



write to ledalab
```{r}
colnames(dt_result4)


dt_time_trig_gsr<- dt_result4[, c(13,51,63)]
colnames(dt_time_trig_gsr)

# unique(dt_result2$participant)

dt_time_trig_gsr<- dt_time_trig_gsr%>%
  arrange(unixtime_start)

# writing a table without column names
# write.table(dt_time_trig_gsr, "dt_time_trig_gsr.txt", sep=",",  col.names=FALSE)

unique(dt_time_trig_gsr$trigger)
table(dt_time_trig_gsr$trigger)



write.table(dt_time_trig_gsr, 
            file = "dt_time_trig_gsr_pre_post_trig.txt", sep = "\t",
            row.names = FALSE, 
            col.names = FALSE)
  
    
    # dt_time_trig_gsr_no0$trigger[dt_time_trig_gsr$trigger == 0]<- ""

write.table(dt_time_trig_gsr_no0[,2:3], 
            file = "dt_time_trig_gsr_no0.txt", sep = "\t",
            row.names = FALSE, 
            col.names = FALSE)

```

combine the ledalab output with behavioural data and check if we have what we would expect

3. Plot Physio data in R and Ledalab

```{r}

dta_leda_era_110 <- read_delim("dt_time_trig_gsr_pre_post_trig_era.txt")

data_psypy1



dta_leda_era_110<- dta_leda_era_110%>%
  janitor::clean_names()

dta_leda_era_110$trigger<- as.numeric(dta_leda_era_110$event_nid)

dta_leda_era_110%>%
  mutate(pre_post_stim = if_else(dta_leda_era_110$trigger<100, "stim",
                                 if_else(dta_leda_era_110$trigger>100 & dta_leda_era_110$trigger< 900, 
                                         "post", "pre")
                                 ))%>%
  mutate(cda_amp_sum_z = scale(cda_amp_sum))%>%
   mutate(cda_amp_sum_z_post = lead(cda_amp_sum_z))%>%
  subset(pre_post_stim = "stim")%>%
 ggplot(aes(pre_post_stim, cda_amp_sum_z))+
  stat_summary(geom = "pointrange")


dta_leda_era_110%>%
  mutate(pre_post_stim = if_else(dta_leda_era_110$trigger<100, "stim",
                                 if_else(dta_leda_era_110$trigger>100 & dta_leda_era_110$trigger< 900, 
                                         "post", "pre")
                                 ))%>%
  mutate(cda_amp_sum_z = scale(cda_amp_sum))%>%
   mutate(cda_amp_sum_z_post = lead(cda_amp_sum_z))%>%
  subset(pre_post_stim = "stim")%>%
 ggplot(aes(cda_amp_sum_z, cda_amp_sum_z_post))+
  geom_point()+
  geom_smooth(method = 'lm', se = F)


dta_leda_era_110%>%
  mutate(pre_post_stim = if_else(dta_leda_era_110$trigger<100, "stim",
                                 if_else(dta_leda_era_110$trigger>100 & dta_leda_era_110$trigger< 900, 
                                         "post", "pre")
                                 ))%>%
  mutate(cda_amp_sum_z = scale(cda_amp_sum))%>%
   mutate(cda_amp_sum_z_post = lag(cda_amp_sum_z))%>%
  subset(pre_post_stim = "stim")%>%
 ggplot(aes(cda_amp_sum_z, cda_amp_sum_z_post))+
  geom_point()+
  geom_smooth(method = 'lm', se = F)

```

```{r}

dta_leda_era_110$trigger
table(dta_leda_era_110$trigger)


dt_result4$trigger

dta_leda_era_110_v1<- left_join(dta_leda_era_110,dt_result4)


# plot emotion category vs SCR
dta_leda_era_110_v1%>%
  # subset(new_variable == TRUE)%>%
  # subset(pre_stim_start_true == TRUE)%>%
  subset(resp_start_true == TRUE)%>%
  mutate(cda_phasic_max_z = scale(cda_scr))%>%
  ggplot(aes(emotioncategory, cda_phasic_max_z))+
  # geom_point()+
  stat_summary(geom = "pointrange")


dta_leda_era_110_v1%>%
  mutate(cda_phasic_max_z = scale(cda_scr))%>%
  ggplot(aes(emotioncategory, cda_phasic_max_z))+
  # geom_point()+
  stat_summary(geom = "pointrange")


# tonic
dta_leda_era_110_v1%>%
  mutate(cda_tonic_z = scale(cda_tonic))%>%
  ggplot(aes(emotioncategory, cda_tonic_z))+
  # geom_point()+
  stat_summary(geom = "pointrange")


# plot intensity vs SCR for response
dta_leda_era_110_v1%>%
  # mutate(cda_tonic_z = scale(cda_tonic))%>%
    mutate(cda_phasic_max_z = scale(cda_scr))%>%
  mutate(int_slider_response_z = scale(int_slider_response))%>%
    subset(new_variable == TRUE)%>%# stim
  # subset(pre_stim_start_true == TRUE)%>%

  # subset(resp_start_true == TRUE)%>%
  ggplot(aes(int_slider_response_z, cda_phasic_max_z))+
  geom_point()+
  geom_smooth(method = 'lm')+
  ggpubr::stat_cor()
  stat_summary(geom = "pointrange")
          
  
  
dta_leda_era_110_v1%>%
  mutate(cda_phasic_max_z = scale(cda_scr))%>%
  mutate(val_slider_response_z = scale(val_slider_response))%>%
 
  ggplot(aes(val_slider_response_z, cda_phasic_max_z))+
  geom_point()+
  geom_smooth(method = 'lm')+
  ggpubr::stat_cor()
  stat_summary(geom = "pointrange")
  
  
  
          

```

